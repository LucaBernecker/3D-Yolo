{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd172667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 8, 20, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the 3D FFCM module\n",
    "class FFCM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(FFCM, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels // 2, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.conv2 = nn.Conv3d(out_channels // 2, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "        # Batch Normalization\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels // 2)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "        # Activation\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        # Apply second convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define the 3D Basic Block\n",
    "class BasicBlock3D(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Define the 3D ResNet model with reduced complexity (layer4 removed)\n",
    "class ResNet3D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1):\n",
    "        super(ResNet3D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.ffcm = FFCM(1, 64)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # Adjust MaxPool3d stride to 2\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Adjust strides in layers to achieve desired output size\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=2)  # Changed stride from 1 to 2\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)  # Keep stride as 2\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=1)  # Changed stride from 2 to 1\n",
    "        # Removed layer4\n",
    "\n",
    "        # Adjusted the input channels to the final convolution layer\n",
    "        self.conv = nn.Conv3d(256 * block.expansion, (7 + num_classes), kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # Activation functions\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Adaptive pooling to get desired grid size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool3d((20, 20, 20))\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv3d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ffcm(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        # Removed layer4 forward pass\n",
    "\n",
    "        x = self.conv(x)  # Output shape: (batch_size, 7 + num_classes, D, H, W)\n",
    "\n",
    "        # Apply adaptive pooling to get grid size 20x20x20\n",
    "        x = self.adaptive_pool(x)\n",
    "\n",
    "        # Apply activation functions to appropriate channels\n",
    "        bbox_pred = self.tanh(x[:, :6, :, :, :])  # Bounding box coordinates between -1 and 1\n",
    "        obj_pred = x[:, 6:7, :, :, :]  # Objectness score (raw output for BCEWithLogitsLoss)\n",
    "        class_pred = x[:, 7:, :, :, :]  # Class scores (raw output for CrossEntropyLoss)\n",
    "\n",
    "        # Concatenate predictions\n",
    "        predictions = torch.cat([bbox_pred, obj_pred, class_pred], dim=1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# The rest of your code remains the same...\n",
    "\n",
    "# Define the custom 3D NMS function\n",
    "def custom_3d_nms(boxes, scores, threshold):\n",
    "    if boxes.size(0) == 0:\n",
    "        return torch.empty(0, dtype=torch.int64, device=boxes.device)\n",
    "\n",
    "    # Sort scores in descending order\n",
    "    scores, idxs = scores.sort(descending=True)\n",
    "    boxes = boxes[idxs]\n",
    "\n",
    "    keep = []\n",
    "    while boxes.size(0) > 0:\n",
    "        keep.append(idxs[0].item())\n",
    "        if boxes.size(0) == 1:\n",
    "            break\n",
    "\n",
    "        iou = compute_3d_iou(boxes[0:1], boxes[1:])\n",
    "        boxes = boxes[1:][iou <= threshold]\n",
    "        idxs = idxs[1:][iou <= threshold]\n",
    "\n",
    "    return torch.tensor(keep, dtype=torch.long, device=boxes.device)\n",
    "\n",
    "# Define the 3D IoU computation\n",
    "def compute_3d_iou(box1, boxes):\n",
    "    inter_xmin = torch.max(box1[:, 0], boxes[:, 0])\n",
    "    inter_ymin = torch.max(box1[:, 1], boxes[:, 1])\n",
    "    inter_zmin = torch.max(box1[:, 2], boxes[:, 2])\n",
    "    inter_xmax = torch.min(box1[:, 3], boxes[:, 3])\n",
    "    inter_ymax = torch.min(box1[:, 4], boxes[:, 4])\n",
    "    inter_zmax = torch.min(box1[:, 5], boxes[:, 5])\n",
    "\n",
    "    inter_dims = (inter_xmax - inter_xmin).clamp(min=0) * \\\n",
    "                 (inter_ymax - inter_ymin).clamp(min=0) * \\\n",
    "                 (inter_zmax - inter_zmin).clamp(min=0)\n",
    "\n",
    "    box1_vol = (box1[:, 3] - box1[:, 0]) * \\\n",
    "               (box1[:, 4] - box1[:, 1]) * \\\n",
    "               (box1[:, 5] - box1[:, 2])\n",
    "\n",
    "    boxes_vol = (boxes[:, 3] - boxes[:, 0]) * \\\n",
    "                (boxes[:, 4] - boxes[:, 1]) * \\\n",
    "                (boxes[:, 5] - boxes[:, 2])\n",
    "\n",
    "    union = box1_vol + boxes_vol - inter_dims\n",
    "    iou = inter_dims / union\n",
    "\n",
    "    return iou\n",
    "\n",
    "# Define the Detect class for inference\n",
    "class Detect(nn.Module):\n",
    "    def __init__(self, num_classes, conf_threshold=0.5, nms_threshold=0.4):\n",
    "        super(Detect, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.nms_threshold = nms_threshold\n",
    "\n",
    "    def forward(self, predictions):\n",
    "        batch_size = predictions.size(0)\n",
    "        D, H, W = predictions.size(2), predictions.size(3), predictions.size(4)\n",
    "\n",
    "        # Reshape predictions\n",
    "        predictions = predictions.permute(0, 2, 3, 4, 1).contiguous()  # (batch_size, D, H, W, 7 + num_classes)\n",
    "        predictions = predictions.view(batch_size, -1, 7 + self.num_classes)  # (batch_size, num_voxels, 7 + num_classes)\n",
    "\n",
    "        # Extract components\n",
    "        pred_bboxes = predictions[:, :, :6]  # (batch_size, num_voxels, 6)\n",
    "        pred_obj_conf = torch.sigmoid(predictions[:, :, 6])  # Apply sigmoid to objectness score\n",
    "        pred_class_scores = torch.sigmoid(predictions[:, :, 7:])  # Apply sigmoid to class scores\n",
    "\n",
    "        # Multiply objectness confidence with class probabilities\n",
    "        pred_scores = pred_obj_conf.unsqueeze(-1) * pred_class_scores  # (batch_size, num_voxels, num_classes)\n",
    "\n",
    "        detections = []\n",
    "        for batch_idx in range(batch_size):\n",
    "            # Filter out low confidence predictions\n",
    "            mask = pred_obj_conf[batch_idx] > self.conf_threshold\n",
    "            if mask.sum() == 0:\n",
    "                detections.append(None)\n",
    "                continue\n",
    "\n",
    "            boxes = pred_bboxes[batch_idx][mask]\n",
    "            scores = pred_scores[batch_idx][mask]\n",
    "            obj_conf = pred_obj_conf[batch_idx][mask]\n",
    "\n",
    "            # For each class, perform NMS\n",
    "            boxes_list = []\n",
    "            scores_list = []\n",
    "            labels_list = []\n",
    "            for cls in range(self.num_classes):\n",
    "                cls_scores = scores[:, cls]\n",
    "                cls_mask = cls_scores > self.conf_threshold\n",
    "                if cls_mask.sum() == 0:\n",
    "                    continue\n",
    "                cls_boxes = boxes[cls_mask]\n",
    "                cls_scores = cls_scores[cls_mask]\n",
    "\n",
    "                # Perform NMS\n",
    "                keep = custom_3d_nms(cls_boxes, cls_scores, self.nms_threshold)\n",
    "                cls_boxes = cls_boxes[keep]\n",
    "                cls_scores = cls_scores[keep]\n",
    "                cls_labels = torch.full((len(keep),), cls, dtype=torch.int64, device=predictions.device)\n",
    "\n",
    "                boxes_list.append(cls_boxes)\n",
    "                scores_list.append(cls_scores)\n",
    "                labels_list.append(cls_labels)\n",
    "\n",
    "            if boxes_list:\n",
    "                detections.append({\n",
    "                    'boxes': torch.cat(boxes_list),\n",
    "                    'scores': torch.cat(scores_list),\n",
    "                    'labels': torch.cat(labels_list)\n",
    "                })\n",
    "            else:\n",
    "                detections.append(None)\n",
    "\n",
    "        return detections\n",
    "\n",
    "# Define the YOLO loss function\n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, num_classes, obj_weight=0.5, noobj_weight=1.0, class_weight=0.5):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.obj_weight = obj_weight\n",
    "        self.noobj_weight = noobj_weight\n",
    "        self.class_weight = class_weight\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([obj_weight]))\n",
    "        self.bce_loss_noobj = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([noobj_weight]))\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        batch_size = predictions.size(0)\n",
    "        D, H, W = predictions.size(2), predictions.size(3), predictions.size(4)\n",
    "\n",
    "        # Reshape predictions and targets\n",
    "        predictions = predictions.permute(0, 2, 3, 4, 1).contiguous()  # (batch_size, D, H, W, 7 + num_classes)\n",
    "        predictions = predictions.view(batch_size, -1, 7 + self.num_classes)  # (batch_size, num_voxels, 7 + num_classes)\n",
    "        targets = targets.permute(0, 2, 3, 4, 1).contiguous()\n",
    "        targets = targets.view(batch_size, -1, 7 + self.num_classes)\n",
    "\n",
    "        # Extract components\n",
    "        pred_bboxes = predictions[:, :, :6]  # (batch_size, num_voxels, 6)\n",
    "        pred_obj_conf = predictions[:, :, 6]  # (batch_size, num_voxels)\n",
    "        pred_class = predictions[:, :, 7:]    # (batch_size, num_voxels, num_classes)\n",
    "\n",
    "        true_bboxes = targets[:, :, :6]\n",
    "        true_obj_conf = targets[:, :, 6]\n",
    "        true_class = targets[:, :, 7:].argmax(dim=2)  # Assuming one-hot encoded classes\n",
    "\n",
    "        # Mask for cells containing objects\n",
    "        obj_mask = true_obj_conf == 1\n",
    "        noobj_mask = true_obj_conf == 0\n",
    "\n",
    "        # Bounding box loss (only for cells with objects)\n",
    "        if obj_mask.sum() > 0:\n",
    "            box_loss = self.mse_loss(pred_bboxes[obj_mask], true_bboxes[obj_mask])\n",
    "            class_loss = self.ce_loss(pred_class[obj_mask], true_class[obj_mask])\n",
    "            obj_loss = self.bce_loss(pred_obj_conf[obj_mask], true_obj_conf[obj_mask])\n",
    "        else:\n",
    "            box_loss = torch.tensor(0.0, device=predictions.device)\n",
    "            class_loss = torch.tensor(0.0, device=predictions.device)\n",
    "            obj_loss = torch.tensor(0.0, device=predictions.device)\n",
    "\n",
    "        # Objectness loss for cells without objects\n",
    "        if noobj_mask.sum() > 0:\n",
    "            noobj_loss = self.bce_loss_noobj(pred_obj_conf[noobj_mask], true_obj_conf[noobj_mask])\n",
    "        else:\n",
    "            noobj_loss = torch.tensor(0.0, device=predictions.device)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = box_loss + obj_loss + noobj_loss + class_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "# Example model and loss initialization\n",
    "model = ResNet3D(BasicBlock3D, [2, 2, 2], num_classes=1)  # Adjusted layers parameter\n",
    "loss_fn = YoloLoss(num_classes=1)  # Adjust num_classes here as well\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define input dimensions\n",
    "D_in, H_in, W_in = 204, 204, 204  # Input dimensions as per your requirement\n",
    "input_dims = (D_in, H_in, W_in)\n",
    "\n",
    "# Verify the output dimensions\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a dummy input tensor\n",
    "    input_tensor = torch.randn(1, 1, D_in, H_in, W_in)\n",
    "    output = model(input_tensor)\n",
    "    print(f\"Output shape: {output.shape}\")  # Should be (1, 8, 20, 20, 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c648f5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9172c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'#100140': [[101.5, 85.0, 97.0, 9, 16, 16]], '#100151': [[120.0, 89.0, 87.5, 30, 12, 19]], '#100152': [[106.5, 79.5, 127.0, 11, 13, 14]], '#100171': [[64.0, 71.5, 107.0, 26, 13, 18]], '#100205': [[137.5, 70.0, 106.5, 21, 24, 29]], '#100218': [[121.0, 62.0, 112.0, 24, 12, 14]], '#100231': [[73.0, 96.5, 83.0, 18, 13, 14]], '#100234': [[82.5, 86.0, 86.5, 23, 12, 11]], '#100242': [[50.5, 87.5, 114.0, 21, 21, 22]], '#100277': [[122.5, 87.0, 130.5, 19, 20, 15]], '#100305': [[81.0, 86.0, 88.0, 18, 14, 16]], '#100316': [[182.5, 82.5, 111.0, 13, 13, 12]], '#100324': [[81.5, 93.0, 85.0, 19, 22, 28]], '#100326': [[99.5, 85.5, 109.5, 11, 19, 17]], '#100394': [[167.5, 93.0, 103.0, 11, 12, 12]], '#100428': [[120.5, 83.0, 116.5, 11, 10, 13]], '#100455': [[52.5, 82.0, 107.5, 13, 14, 19]], '#100460': [[186.0, 95.0, 111.0, 16, 12, 16]], '#100517': [[158.0, 86.0, 102.0, 32, 18, 20]], '#100536': [[32.5, 80.5, 95.5, 9, 27, 15]], '#100556': [[166.0, 87.5, 103.0, 14, 9, 18]], '#100601': [[147.0, 64.5, 142.5, 22, 13, 13]], '#100643': [[17.0, 79.0, 102.0, 16, 14, 14]], '#100649': [[97.5, 84.0, 102.5, 13, 14, 17]], '#100660': [[75.0, 84.5, 87.5, 16, 15, 13]], '#100673': [[134.5, 76.5, 103.0, 27, 19, 30]], '#100680': [[80.0, 80.0, 90.0, 24, 20, 24]], '#100700': [[98.5, 42.0, 110.5, 27, 28, 25]], '#100723': [[83.5, 92.0, 82.5, 27, 22, 33]], '#100739': [[17.0, 89.5, 108.0, 14, 15, 14]], '#100818': [[155.5, 85.0, 114.5, 23, 24, 17]], '#100873': [[110.5, 85.0, 95.0, 13, 20, 16]], '#100880': [[121.5, 66.0, 100.0, 19, 16, 12]], '#100915': [[112.0, 88.0, 78.0, 14, 16, 16]], '#100988': [[39.5, 95.5, 92.5, 19, 17, 17]], '#101026': [[53.5, 72.0, 113.0, 13, 18, 20]], '#101043': [[160.5, 86.0, 113.0, 7, 10, 18]], '#101054': [[28.5, 111.0, 97.5, 11, 12, 13]], '#101085': [[74.5, 96.5, 94.0, 19, 17, 18]], '#101118': [[120.0, 78.0, 93.5, 22, 12, 17]], '#101122': [[128.0, 66.5, 104.5, 12, 15, 13]], '#101140': [[125.0, 70.0, 97.5, 12, 14, 13]], '#101148': [[106.5, 55.5, 103.5, 9, 15, 17]], '#101157': [[46.5, 86.5, 89.0, 19, 9, 14]], '#101158': [[169.5, 78.0, 112.0, 17, 12, 14]], '#101194': [[184.0, 78.5, 108.0, 20, 13, 16]], '#101220': [[130.0, 68.0, 106.5, 12, 14, 15]], '#101234': [[120.5, 86.0, 91.0, 25, 20, 14]], '#101263': [[121.5, 87.0, 96.0, 17, 14, 32]], '#101272': [[19.5, 82.0, 100.0, 15, 8, 10]], '#101288': [[41.5, 94.5, 97.0, 13, 17, 14], [148.0, 72.0, 106.5, 12, 12, 13]], '#101298': [[155.0, 96.0, 116.0, 12, 14, 14]], '#101302': [[41.5, 71.5, 104.5, 11, 15, 17]], '#101309': [[98.0, 78.0, 112.0, 14, 12, 10]], '#101331': [[97.5, 82.5, 106.5, 9, 13, 15]], '#101365': [[83.5, 82.0, 122.0, 17, 16, 16]], '#101393': [[182.5, 91.0, 106.0, 13, 26, 20]], '#101426': [[17.5, 77.5, 93.0, 11, 25, 10], [110.5, 51.0, 109.5, 9, 12, 11]], '#101493': [[103.0, 41.0, 110.0, 24, 20, 20]], '#101495': [[151.0, 74.5, 115.5, 10, 19, 13]], '#101508': [[40.5, 91.5, 104.5, 13, 19, 13]], '#101516': [[12.0, 100.0, 130.0, 10, 18, 12]], '#101562': [[23.5, 88.5, 109.0, 17, 17, 14]], '#101601': [[171.0, 91.5, 100.5, 14, 15, 17]], '#101675': [[80.5, 84.0, 86.5, 19, 14, 17]], '#101735': [[42.0, 74.0, 121.0, 18, 12, 10]], '#101782': [[123.5, 83.5, 90.0, 17, 15, 12]], '#101842': [[42.0, 76.5, 93.0, 20, 23, 20]], '#101885': [[100.5, 85.0, 118.5, 13, 24, 23]], '#101892': [[145.0, 75.0, 112.5, 16, 12, 13]], '#101990': [[118.5, 82.5, 81.5, 35, 15, 19]], '#101991': [[106.0, 90.5, 113.0, 16, 15, 16]], '#101995': [[121.0, 72.0, 100.5, 14, 16, 11]], '#101998': [[120.0, 83.5, 103.0, 26, 17, 16]], '#102024': [[75.5, 64.5, 103.5, 19, 13, 21]], '#102084': [[48.0, 88.5, 98.0, 20, 25, 12]], '#102141': [[102.0, 86.5, 116.5, 12, 15, 13]], '#102158': [[157.5, 87.0, 108.0, 17, 16, 18]], '#102200': [[116.0, 92.0, 88.5, 30, 20, 33]], '#102201': [[117.5, 85.0, 96.0, 23, 16, 12]], '#102202': [[102.5, 80.0, 108.0, 15, 18, 10]], '#102228': [[156.0, 86.5, 105.5, 14, 9, 13]], '#102283': [[175.5, 85.0, 98.0, 13, 18, 16]], '#102333': [[114.0, 92.0, 100.0, 20, 20, 20]], '#102348': [[78.0, 83.5, 96.5, 14, 19, 13]], '#102391': [[128.5, 74.5, 108.0, 21, 17, 16]], '#102418': [[126.5, 78.5, 94.5, 19, 25, 17]], '#102427': [[78.5, 93.5, 89.5, 21, 13, 17]], '#102442': [[75.0, 73.5, 100.0, 16, 13, 16]], '#102455': [[152.0, 77.0, 92.5, 16, 12, 17]], '#102496': [[170.5, 82.0, 108.0, 21, 16, 16]], '#102564': [[127.5, 80.5, 113.5, 23, 21, 15]], '#102590': [[157.0, 85.5, 101.0, 14, 9, 16], [123.5, 85.0, 83.5, 19, 14, 9]], '#102595': [[119.5, 80.5, 88.0, 15, 21, 10], [122.0, 80.5, 98.5, 18, 11, 13]], '#102601': [[96.0, 88.5, 98.5, 16, 15, 11]], '#102609': [[31.0, 89.0, 104.5, 22, 18, 21]], '#102638': [[50.0, 78.0, 121.0, 20, 16, 18]], '#102693': [[167.0, 90.0, 83.5, -84, 16, 17]], '#102720': [[29.0, 93.0, 97.5, 14, 14, 11]], '#102761': [[76.5, 87.0, 89.0, 17, 14, 18]], '#102764': [[81.5, 82.0, 91.5, 19, 20, 11]], '#102797': [[79.5, 94.0, 91.0, 13, 16, 14]], '#102835': [[167.5, 97.0, 94.5, 21, 14, 13]], '#102840': [[120.0, 88.0, 86.0, 18, 12, 16]], '#102864': [[122.0, 89.5, 86.0, 20, 21, 14], [78.0, 90.0, 88.5, 20, 14, 13], [192.5, 40.0, 137.0, 17, 16, 12]], '#102886': [[45.0, 82.5, 102.0, 8, 23, 10]], '#102935': [[152.0, 81.0, 112.0, 18, 20, 16]], '#102959': [[75.5, 85.5, 94.5, 19, 15, 9]], '#102974': [[118.5, 85.5, 80.5, 17, 19, 19]]}\n"
     ]
    }
   ],
   "source": [
    "def convert_bboxes(bboxes):\n",
    "    # Check if bboxes is a list of bounding boxes\n",
    "    if isinstance(bboxes[0][0], list):\n",
    "        # Convert each bounding box in the list\n",
    "        return [convert_bboxes(bbox) for bbox in bboxes]\n",
    "    else:\n",
    "        # Convert a single bounding box\n",
    "        x_min, x_max = bboxes[0]\n",
    "        y_min, y_max = bboxes[1]\n",
    "        z_min, z_max = bboxes[2]\n",
    "\n",
    "        x_center = (x_min + x_max) / 2\n",
    "        y_center = (y_min + y_max) / 2\n",
    "        z_center = (z_min + z_max) / 2\n",
    "\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "        depth = z_max - z_min\n",
    "\n",
    "        return [x_center, y_center, z_center, width, height, depth]\n",
    "# Example usage\n",
    "bounding_boxes_with_comments = {\n",
    "    \"#100140\": [[[97, 106], [77, 93], [89, 105]]],\n",
    "    \"#100151\": [[[105, 135], [83, 95], [78, 97]]],\n",
    "    \"#100152\": [[[101, 112], [73, 86], [120, 134]]],\n",
    "    \"#100171\": [[[51, 77], [65, 78], [98, 116]]],\n",
    "    \"#100205\": [[[127, 148], [58, 82], [92, 121]]],\n",
    "    \"#100218\": [[[109, 133], [56, 68], [105, 119]]],\n",
    "    \"#100231\": [[[64, 82], [90, 103], [76, 90]]],\n",
    "    \"#100234\": [[[71, 94], [80, 92], [81, 92]]],\n",
    "    \"#100242\": [[[40, 61], [77, 98], [103, 125]]],\n",
    "    \"#100277\": [[[113, 132], [77, 97], [123, 138]]],\n",
    "    \"#100305\": [[[72, 90], [79, 93], [80, 96]]],\n",
    "    \"#100316\": [[[176, 189], [76, 89], [105, 117]]],\n",
    "    \"#100324\": [[[72, 91], [82, 104], [71, 99]]],\n",
    "    \"#100326\": [[[94, 105], [76, 95], [101, 118]]],\n",
    "    \"#100394\": [[[162, 173], [87, 99], [97, 109]]],\n",
    "   # \"#100248\": [[[115, 129], [76, 94], [110, 124]]],\n",
    "    \"#100428\": [[[115,126],[78,88],[110,123]]],\n",
    "    \"#100455\": [[[46, 59], [75, 89], [98, 117]]],\n",
    "    \"#100460\": [[[178, 194], [89, 101], [103, 119]]],\n",
    "    \"#100517\": [[[142, 174], [77, 95], [92, 112]]],\n",
    "    \"#100536\": [[[28, 37], [67, 94], [88, 103]]],\n",
    "    \"#100556\": [[[159, 173], [83, 92], [94, 112]]],\n",
    "    \"#100601\": [[[136, 158], [58, 71], [136, 149]]],\n",
    "    \"#100643\": [[[9, 25], [72, 86], [95, 109]]],\n",
    "    \"#100649\": [[[91, 104], [77, 91], [94, 111]]],\n",
    "    \"#100660\": [[[67, 83], [77, 92], [81, 94]]],\n",
    "    \"#100673\": [[[121, 148], [67, 86], [88, 118]]],\n",
    "    \"#100680\": [[[68, 92], [70, 90], [78, 102]]],\n",
    "    \"#100700\": [[[85, 112], [28, 56], [98, 123]]],\n",
    "    \"#100723\": [[[70, 97], [81, 103], [66, 99]]],\n",
    "    \"#100739\": [[[10, 24], [82, 97], [101, 115]]],\n",
    "    \"#100818\": [[[144, 167], [73, 97], [106, 123]]],\n",
    "    \"#100873\": [[[104, 117], [75, 95], [87, 103]]],\n",
    "    \"#100880\": [[[112, 131], [58, 74], [94, 106]]],\n",
    "    \"#100915\": [[[105, 119], [80, 96], [70, 86]]],\n",
    "    \"#100988\": [[[30, 49], [87, 104], [84, 101]]],\n",
    "    \"#101026\": [[[47, 60], [63, 81], [103, 123]]],\n",
    "    \"#101043\": [[[157, 164], [81, 91], [104, 122]]],\n",
    "    \"#101054\": [[[23, 34], [105, 117], [91, 104]]],\n",
    "    \"#101085\": [[[65, 84], [88, 105], [85, 103]]],\n",
    "    \"#101118\": [[[109, 131], [72, 84], [85, 102]]],\n",
    "    \"#101122\": [[[122, 134], [59, 74], [98, 111]]],\n",
    "    \"#101140\": [[[119, 131], [63, 77], [91, 104]]],\n",
    "    \"#101148\": [[[102, 111], [48, 63], [95, 112]]],\n",
    "    \"#101157\": [[[37, 56], [82, 91], [82, 96]]],\n",
    "    \"#101158\": [[[161, 178], [72, 84], [105, 119]]],\n",
    "    \"#101194\": [[[174, 194], [72, 85], [100, 116]]],\n",
    "    \"#101220\": [[[124, 136], [61, 75], [99, 114]]],\n",
    "    \"#101234\": [[[108, 133], [76, 96], [84, 98]]],\n",
    "    \"#101263\": [[[113, 130], [80, 94], [80, 112]]],\n",
    "    \"#101272\": [[[12, 27], [78, 86], [95, 105]]],\n",
    "    \"#101288\": [[[35, 48], [86, 103], [90, 104]],[[142,154], [66,78], [100,113]]],\n",
    "    \"#101298\": [[[149, 161], [89, 103], [109, 123]]],\n",
    "    \"#101302\": [[[36, 47], [64, 79], [96, 113]]],\n",
    "    \"#101309\": [[[91, 105], [72, 84], [107, 117]]],\n",
    "    \"#101331\": [[[93, 102], [76, 89], [99, 114]]],\n",
    "    \"#101365\": [[[75, 92], [74, 90], [114, 130]]],\n",
    "    \"#101393\": [[[176, 189], [78, 104], [96, 116]]],\n",
    "    \"#101426\": [[[12, 23], [65, 90], [88, 98]],[[106,115],[45,57],[104,115]]],  \n",
    "    \"#101493\": [[[91, 115], [31, 51], [100, 120]]],\n",
    "    \"#101495\": [[[146, 156], [65, 84], [109, 122]]],\n",
    "    \"#101508\": [[[34, 47], [82, 101], [98, 111]]],\n",
    "    \"#101516\": [[[7, 17], [91, 109], [124, 136]]],\n",
    "    \"#101562\": [[[15, 32], [80, 97], [102, 116]]],\n",
    "    \"#101601\": [[[164, 178], [84, 99], [92, 109]]],\n",
    "    \"#101675\": [[[71, 90], [77, 91], [78, 95]]],\n",
    "    \"#101735\": [[[33, 51], [68, 80],[116,126]]],\n",
    "    \"#101782\": [[[115, 132], [76, 91], [84, 96]]],\n",
    "    \"#101842\": [[[32, 52], [65, 88], [83, 103]]],\n",
    "    \"#101885\": [[[94, 107], [73, 97], [107, 130]]],\n",
    "    \"#101892\": [[[137, 153], [69, 81], [106, 119]]],\n",
    "    \"#101990\": [[[101, 136], [75, 90], [72, 91]]],\n",
    "    \"#101991\": [[[98, 114], [83, 98], [105, 121]]],\n",
    "    \"#101995\": [[[114, 128], [64, 80], [95, 106]]],\n",
    "    \"#101998\": [[[107, 133], [75, 92], [95, 111]]],\n",
    "    \"#102024\": [[[66, 85], [58, 71], [93, 114]]],\n",
    "    \"#102084\": [[[38, 58], [76, 101], [92, 104]]],\n",
    "    \"#102141\": [[[96, 108], [79, 94], [110, 123]]],\n",
    "    \"#102158\": [[[149, 166], [79, 95], [99, 117]]],\n",
    "    \"#102200\": [[[101, 131], [82, 102], [72, 105]]],\n",
    "    \"#102201\": [[[106, 129], [77, 93], [90, 102]]],\n",
    "    \"#102202\": [[[95, 110], [71, 89], [103, 113]]],\n",
    "    \"#102228\": [[[149, 163], [82, 91], [99, 112]]],\n",
    "    \"#102283\": [[[169, 182], [76, 94], [90, 106]]],\n",
    "    \"#102333\": [[[104, 124], [82, 102], [90, 110]]],\n",
    "    \"#102348\": [[[71, 85], [74, 93], [90, 103]]],\n",
    "    \"#102391\": [[[118, 139], [66, 83], [100, 116]]],\n",
    "    \"#102418\": [[[117, 136], [66, 91], [86, 103]]],\n",
    "    \"#102427\": [[[68, 89], [87, 100], [81, 98]]],\n",
    "    \"#102442\": [[[67, 83], [67, 80], [92, 108]]],\n",
    "    \"#102455\": [[[144, 160], [71, 83], [84, 101]]],\n",
    "    \"#102496\": [[[160, 181], [74, 90], [100, 116]]],\n",
    "    \"#102564\": [[[116, 139], [70, 91], [106, 121]]],\n",
    "    \"#102590\": [[[150, 164], [81, 90], [93, 109]],[[114,133],[78,92],[79,88]]],  # and [[118, 136], [82, 93], [77, 91]]\n",
    "    \"#102595\": [[[112, 127], [70, 91], [83, 93]],[[113, 131], [75, 86], [92, 105]]],  # and [[113, 131], [75, 86], [92, 105]]\n",
    "    \"#102601\": [[[88, 104], [81, 96], [93, 104]]],\n",
    "    \"#102609\": [[[20, 42], [80, 98], [94, 115]]],\n",
    "    \"#102638\": [[[40, 60], [70, 86], [112, 130]]],\n",
    "    \"#102693\": [[[209, 125], [82, 98], [75, 92]]],\n",
    "    \"#102720\": [[[22, 36], [86, 100], [92, 103]]],\n",
    "    \"#102761\": [[[68, 85], [80, 94], [80, 98]]],\n",
    "    \"#102764\": [[[72, 91], [72, 92], [86, 97]]],\n",
    "    \"#102797\": [[[73, 86], [86, 102], [84, 98]]],\n",
    "    \"#102835\": [[[157, 178], [90, 104], [88, 101]]],\n",
    "    \"#102840\": [[[111, 129], [82, 94], [78, 94]]],\n",
    "    \"#102864\": [[[112, 132], [79, 100], [79, 93]],[[68, 88], [83, 97], [82, 95]],[[184, 201], [32, 48], [131, 143]]],  # and [[68, 88], [83, 97], [82, 95]], [[184, 201], [32, 48], [131, 143]]\n",
    "    \"#102886\": [[[41, 49], [71, 94], [97, 107]]],\n",
    "    \"#102935\": [[[143, 161], [71, 91], [104, 120]]],\n",
    "    \"#102959\": [[[66, 85], [78, 93], [90, 99]]],\n",
    "    \"#102974\": [[[110, 127], [76, 95], [71, 90]]]\n",
    "}\n",
    "\n",
    "\n",
    "converted_bboxes_with_comments = {}\n",
    "\n",
    "for key, bboxes in bounding_boxes_with_comments.items():\n",
    "    # Convert each list of bounding boxes or single bounding box\n",
    "    converted_bboxes_with_comments[key] = convert_bboxes(bboxes)\n",
    "\n",
    "print(converted_bboxes_with_comments)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b1d33f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train Shape: torch.Size([109, 1, 204, 204, 204])\n",
      "Targets Shape: torch.Size([109, 8, 13, 13, 13])\n",
      "Targets Shape: torch.Size([109, 8, 13, 13, 13])\n",
      "Batch 1\n",
      "Input data shape: torch.Size([1, 1, 204, 204, 204])\n",
      "Predictions shape: torch.Size([1, 8, 13, 13, 13])\n",
      "Target data shape: torch.Size([1, 8, 13, 13, 13])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 157\u001b[0m\n\u001b[1;32m    154\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(predictions, target_data)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    158\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Print loss for this batch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming the ResNet3D model and YoloLoss class are already defined as per the previous code.\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Directory containing the NIfTI files\n",
    "nifti_directory = '/Users/lucabernecker/Desktop/N128_local/aneu_det'\n",
    "\n",
    "# List all file IDs from bounding box keys\n",
    "file_ids = [key.strip(\"#\") for key in converted_bboxes_with_comments.keys()]\n",
    "\n",
    "# Initialize a list to store the loaded NIfTI images as numpy arrays\n",
    "nifti_images_list = []\n",
    "\n",
    "# Desired output shape (consistent with the input dimensions expected by the model)\n",
    "desired_shape = (204, 204, 204)\n",
    "\n",
    "# Function to pad the images\n",
    "def pad_image(image, target_shape):\n",
    "    pad_width = [(0, max(0, t - s)) for s, t in zip(image.shape, target_shape)]\n",
    "    return np.pad(image, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "# Load and preprocess the NIfTI images\n",
    "for file_id in file_ids:\n",
    "    file_name = f\"cube_{file_id}.nii.gz\"\n",
    "    file_path = os.path.join(nifti_directory, file_name)\n",
    "    \n",
    "    # Load the NIfTI image\n",
    "    nifti_image = nib.load(file_path)\n",
    "    \n",
    "    # Convert the NIfTI image to a numpy array\n",
    "    image_data = nifti_image.get_fdata()\n",
    "    \n",
    "    # Pad the image to the desired shape\n",
    "    padded_image = pad_image(image_data, desired_shape)\n",
    "    \n",
    "    # Append to the list\n",
    "    nifti_images_list.append(padded_image)\n",
    "\n",
    "# Stack all numpy arrays into a single numpy array\n",
    "x_train = np.stack(nifti_images_list, axis=0)\n",
    "\n",
    "# Convert the list of images to a Torch tensor and add a channel dimension\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "x_train = torch.unsqueeze(x_train, 1)  # Shape: (batch_size, 1, 204, 204, 204)\n",
    "print(\"X_Train Shape:\", x_train.shape)\n",
    "\n",
    "# Number of classes (assuming 1 class)\n",
    "num_classes = 1\n",
    "\n",
    "# Grid size for 3D detection (e.g., 13x13x13 grid)\n",
    "grid_size = 13\n",
    "\n",
    "# Initialize the target tensor\n",
    "targets = torch.zeros((len(file_ids), 7 + num_classes, grid_size, grid_size, grid_size))  # Shape: (batch_size, 8, 13, 13, 13)\n",
    "\n",
    "# Prepare the targets\n",
    "for idx, (file_id, boxes) in enumerate(converted_bboxes_with_comments.items()):\n",
    "    for box in boxes:\n",
    "        # Extract center coordinates (x, y, z) and dimensions (width, height, depth)\n",
    "        center_x, center_y, center_z, width, height, depth = box\n",
    "        \n",
    "        # Normalize coordinates to be between 0 and 1\n",
    "        norm_center_x = center_x / desired_shape[0]\n",
    "        norm_center_y = center_y / desired_shape[1]\n",
    "        norm_center_z = center_z / desired_shape[2]\n",
    "        \n",
    "        # Compute which grid cell the center falls into\n",
    "        grid_x = int(norm_center_x * grid_size)\n",
    "        grid_y = int(norm_center_y * grid_size)\n",
    "        grid_z = int(norm_center_z * grid_size)\n",
    "        \n",
    "        # Ensure grid indices are within bounds\n",
    "        grid_x = min(grid_size - 1, max(0, grid_x))\n",
    "        grid_y = min(grid_size - 1, max(0, grid_y))\n",
    "        grid_z = min(grid_size - 1, max(0, grid_z))\n",
    "        \n",
    "        # Encode bounding box coordinates relative to the grid cell\n",
    "        # Compute the offset within the cell\n",
    "        cell_x = norm_center_x * grid_size - grid_x\n",
    "        cell_y = norm_center_y * grid_size - grid_y\n",
    "        cell_z = norm_center_z * grid_size - grid_z\n",
    "        \n",
    "        # Normalize the bounding box dimensions to be relative to the entire image size\n",
    "        norm_width = width / desired_shape[0]\n",
    "        norm_height = height / desired_shape[1]\n",
    "        norm_depth = depth / desired_shape[2]\n",
    "        \n",
    "        # Add bounding box info to the target tensor (grid cell)\n",
    "        targets[idx, 0:6, grid_x, grid_y, grid_z] = torch.tensor(\n",
    "            [cell_x, cell_y, cell_z, norm_width, norm_height, norm_depth]\n",
    "        )\n",
    "        # Set objectness score to 1\n",
    "        targets[idx, 6, grid_x, grid_y, grid_z] = 1\n",
    "        # Set class label (one-hot encoding)\n",
    "        class_label = 0  # Assuming only one class\n",
    "        targets[idx, 7 + class_label, grid_x, grid_y, grid_z] = 1\n",
    "\n",
    "print(\"Targets Shape:\", targets.shape)\n",
    "\n",
    "# Define the model\n",
    "model = ResNet3D(BasicBlock3D, [2, 2, 2, 2], num_classes=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = YoloLoss(num_classes=num_classes)\n",
    "loss_fn.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Move data to device\n",
    "x_train = x_train.to(device)\n",
    "targets = targets.to(device)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 50\n",
    "batch_size = 1\n",
    "num_samples = x_train.shape[0]\n",
    "num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "print(\"Targets Shape:\", targets.shape)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, num_samples)\n",
    "        input_data = x_train[start_idx:end_idx]\n",
    "        target_data = targets[start_idx:end_idx]\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(input_data)\n",
    "        \n",
    "        # Print shapes for debugging\n",
    "        print(f\"Batch {batch_idx+1}\")\n",
    "        print(\"Input data shape:\", input_data.shape)\n",
    "        print(\"Predictions shape:\", predictions.shape)\n",
    "        print(\"Target data shape:\", target_data.shape)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(predictions, target_data)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss for this batch\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{num_batches}], Loss: {loss.item()}\")\n",
    "        \n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17f28a8",
   "metadata": {},
   "source": [
    "### Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e36abc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train Shape: torch.Size([109, 1, 204, 204, 204])\n",
      "Annotated image with bounding box saved to: /Users/lucabernecker/Desktop/N128_local/aneu_det/painted_first_image.nii.gz\n"
     ]
    }
   ],
   "source": [
    "nifti_directory = '/Users/lucabernecker/Desktop/N128_local/aneu_det'\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# List all file IDs directly from converted_bboxes_with_comments keys\n",
    "file_ids = list(converted_bboxes_with_comments.keys())  # Keep the '#' in place\n",
    "\n",
    "# Initialize a list to store the loaded nifti images as numpy arrays\n",
    "nifti_images_list = []\n",
    "\n",
    "# Desired output shape\n",
    "desired_shape = (204, 204, 204)\n",
    "\n",
    "# Function to pad the images\n",
    "def pad_image(image, target_shape):\n",
    "    pad_width = [(0, max(0, t - s)) for s, t in zip(image.shape, target_shape)]\n",
    "    return np.pad(image, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "# Load and preprocess the nifti images\n",
    "for file_id in file_ids:\n",
    "    file_name = f\"cube_{file_id.strip('#')}.nii.gz\"  # Strip the '#' here when loading the file\n",
    "    file_path = os.path.join(nifti_directory, file_name)\n",
    "    \n",
    "    # Load the nifti image\n",
    "    nifti_image = nib.load(file_path)\n",
    "    \n",
    "    # Convert the nifti image to a numpy array\n",
    "    image_data = nifti_image.get_fdata()\n",
    "    \n",
    "    # Pad the image to the desired shape\n",
    "    padded_image = pad_image(image_data, desired_shape)\n",
    "    \n",
    "    # Append to the list\n",
    "    nifti_images_list.append(padded_image)\n",
    "\n",
    "# Stack all numpy arrays into a single numpy array\n",
    "x_train = np.stack(nifti_images_list, axis=0)\n",
    "\n",
    "# Convert the list of images to a NumPy array and then to a Torch tensor\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "x_train = torch.unsqueeze(x_train, 1)  # Add a channel dimension\n",
    "print(\"X_Train Shape:\", np.shape(x_train))\n",
    "\n",
    "# Function to \"paint\" the bounding box\n",
    "def paint_bounding_box(image, bbox, value=1):\n",
    "    \"\"\"Paint the region inside the bounding box in the image with the given intensity value.\"\"\"\n",
    "    center_x, center_y, center_z, width, height, depth = bbox\n",
    "\n",
    "    # Calculate the min and max coordinates from the center and dimensions\n",
    "    x_min = max(0, int(center_x - width / 2))\n",
    "    x_max = min(image.shape[0] - 1, int(center_x + width / 2))\n",
    "    \n",
    "    y_min = max(0, int(center_y - height / 2))\n",
    "    y_max = min(image.shape[1] - 1, int(center_y + height / 2))\n",
    "    \n",
    "    z_min = max(0, int(center_z - depth / 2))\n",
    "    z_max = min(image.shape[2] - 1, int(center_z + depth / 2))\n",
    "\n",
    "    # Paint the bounding box region in the image with the maximum intensity\n",
    "    image[x_min:x_max + 1, y_min:y_max + 1, z_min:z_max + 1] = value\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Assuming the first file ID corresponds to the first image in x_train\n",
    "first_file_id = file_ids[0]  # Use file_id with '#'\n",
    "\n",
    "# Check if the file_id exists in converted_bboxes_with_comments\n",
    "if first_file_id in converted_bboxes_with_comments:\n",
    "    bounding_box = converted_bboxes_with_comments[first_file_id][0]  # Use the first bounding box for this example\n",
    "\n",
    "    # Use the bounding box from the targets for the first image\n",
    "    first_image = x_train[0].squeeze().numpy()\n",
    "\n",
    "    # Paint the bounding box with a higher intensity (representing \"red\" in grayscale)\n",
    "    painted_image = paint_bounding_box(first_image, bounding_box, value=255)\n",
    "\n",
    "    # Create a new NIfTI image with the painted bounding box\n",
    "    painted_nifti = nib.Nifti1Image(painted_image, affine=np.eye(4))\n",
    "\n",
    "    # Save the modified image as a NIfTI file\n",
    "    output_path = os.path.join(nifti_directory, \"painted_first_image.nii.gz\")\n",
    "    nib.save(painted_nifti, output_path)\n",
    "\n",
    "    print(f\"Annotated image with bounding box saved to: {output_path}\")\n",
    "else:\n",
    "    print(f\"File ID {first_file_id} not found in bounding box data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b9fedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
