{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154020f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56c84e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4e2235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7423e4b",
   "metadata": {},
   "source": [
    "# Yolo 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8088d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock3D(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Define the 3D ResNet model\n",
    "class ResNet3D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1, num_anchors=1):\n",
    "        super(ResNet3D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv3d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "   #     print(\"anchors\",num_anchors,\"classes\",num_classes)\n",
    "        self.conv = nn.Conv3d(512 * block.expansion, num_anchors * (7 + num_classes), kernel_size=1,stride=1,padding=2)\n",
    "        self.detect = Detect(num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv3d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.conv(x)\n",
    "  #      print(x.shape,\"final before detect\")\n",
    "        detections, predictions = self.detect(x)\n",
    "        return detections, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdbfca11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchio as tio\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "# Define a 3D BasicBlock for ResNet\n",
    "\n",
    "\n",
    "# Create a Random 3D Dataset\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "# Iterate through the dataset\n",
    " #   print(\"Sample image shape (tensor):\", img.shape)\n",
    " #   print(\"Sample target (x, y, z, w, h, d):\", target)\n",
    "#print(\"box1\",np.shape(box1))\n",
    "#print(\"boxes\",np.shape(boxes))\n",
    "\n",
    "class Detect(nn.Module):\n",
    "    def __init__(self, num_classes, conf_threshold=0.5, nms_threshold=0.4):\n",
    "        super(Detect, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.nms_threshold = nms_threshold\n",
    "\n",
    "    def forward(self, predictions):\n",
    "    #    print(\"predicitions in detect\",predictions.shape)\n",
    "        batch_size = predictions.size(0)\n",
    "        num_channels = predictions.size(1)\n",
    "        grid_size = predictions.size(2)\n",
    "        \n",
    "        # Calculate expected number of channels\n",
    "        num_anchors = 1\n",
    "        num_params_per_anchor = 7 + self.num_classes\n",
    "        expected_channels = batch_size * num_anchors * num_params_per_anchor * grid_size * grid_size * grid_size\n",
    "        \n",
    "        # Reshape predictions correctly\n",
    "        predictions = predictions.view(batch_size, num_anchors, num_params_per_anchor, grid_size, grid_size, grid_size)\n",
    "\n",
    "        bbox_pred = predictions[:, :, :6, :, :, :]  # Adjust for 3D: 6 parameters for bbox\n",
    "        obj_confidence = torch.sigmoid(predictions[:, :, 6, :, :, :])\n",
    "        class_scores = torch.sigmoid(predictions[:, :, 7:, :, :, :])\n",
    "\n",
    "        grid_x = torch.arange(grid_size, dtype=torch.float, device=predictions.device).repeat(grid_size, grid_size, 1)\n",
    "        grid_y = torch.arange(grid_size, dtype=torch.float, device=predictions.device).repeat(grid_size, grid_size, 1).transpose(1, 2)\n",
    "        grid_z = torch.arange(grid_size, dtype=torch.float, device=predictions.device).repeat(grid_size, grid_size, 1).transpose(0, 1)\n",
    "\n",
    "        scaled_anchors = torch.zeros_like(bbox_pred)\n",
    "\n",
    "        anchor_w = [12.0] * num_anchors\n",
    "        anchor_h = [12.0] * num_anchors\n",
    "        anchor_d = [12.0] * num_anchors\n",
    "\n",
    "        for i in range(num_anchors):\n",
    "            scaled_anchors[:, i, 0, :, :, :] = (torch.sigmoid(bbox_pred[:, i, 0, :, :, :]) + grid_x) / grid_size\n",
    "            scaled_anchors[:, i, 1, :, :, :] = (torch.sigmoid(bbox_pred[:, i, 1, :, :, :]) + grid_y) / grid_size\n",
    "            scaled_anchors[:, i, 2, :, :, :] = (torch.sigmoid(bbox_pred[:, i, 2, :, :, :]) + grid_z) / grid_size\n",
    "            scaled_anchors[:, i, 3, :, :, :] = torch.exp(bbox_pred[:, i, 3, :, :, :]) * anchor_w[i] / grid_size\n",
    "            scaled_anchors[:, i, 4, :, :, :] = torch.exp(bbox_pred[:, i, 4, :, :, :]) * anchor_h[i] / grid_size\n",
    "            scaled_anchors[:, i, 5, :, :, :] = torch.exp(bbox_pred[:, i, 5, :, :, :]) * anchor_d[i] / grid_size\n",
    "\n",
    "        x_min = scaled_anchors[:, :, 0, :, :, :] - scaled_anchors[:, :, 3, :, :, :] / 2\n",
    "        y_min = scaled_anchors[:, :, 1, :, :, :] - scaled_anchors[:, :, 4, :, :, :] / 2\n",
    "        z_min = scaled_anchors[:, :, 2, :, :, :] - scaled_anchors[:, :, 5, :, :, :] / 2\n",
    "        x_max = scaled_anchors[:, :, 0, :, :, :] + scaled_anchors[:, :, 3, :, :, :] / 2\n",
    "        y_max = scaled_anchors[:, :, 1, :, :, :] + scaled_anchors[:, :, 4, :, :, :] / 2\n",
    "        z_max = scaled_anchors[:, :, 2, :, :, :] + scaled_anchors[:, :, 5, :, :, :] / 2\n",
    "\n",
    "        detections = []\n",
    "        for batch_idx in range(batch_size):\n",
    "            batch_detections = []\n",
    "            for class_idx in range(self.num_classes):\n",
    "                class_scores_batch = class_scores[batch_idx, :, class_idx, :, :, :]\n",
    "                conf_scores = obj_confidence[batch_idx, :, :, :, :]\n",
    "                scores = conf_scores * class_scores_batch\n",
    "                mask = scores >= self.conf_threshold\n",
    "                scores = scores[mask]\n",
    "                if scores.size(0) == 0:\n",
    "                    continue\n",
    "                x_min_class = x_min[batch_idx, :, :, :, :][mask]\n",
    "                y_min_class = y_min[batch_idx, :, :, :, :][mask]\n",
    "                z_min_class = z_min[batch_idx, :, :, :, :][mask]\n",
    "                x_max_class = x_max[batch_idx, :, :, :, :][mask]\n",
    "                y_max_class = y_max[batch_idx, :, :, :, :][mask]\n",
    "                z_max_class = z_max[batch_idx, :, :, :, :][mask]\n",
    "                boxes = torch.stack((x_min_class, y_min_class, z_min_class, x_max_class, y_max_class, z_max_class), dim=-1)\n",
    "\n",
    "                keep = custom_3d_nms(boxes, scores, self.nms_threshold)\n",
    "\n",
    "                boxes = boxes.view(-1, 6)[keep]\n",
    "                scores = scores[keep]\n",
    "\n",
    "                batch_detections.extend(\n",
    "                    [{\"boxes\": boxes, \"scores\": scores, \"labels\": torch.tensor([class_idx]*len(boxes), device=predictions.device)}]\n",
    "                )\n",
    "            detections.append(batch_detections)\n",
    "\n",
    "        return detections, predictions\n",
    "def custom_3d_nms(boxes, scores, threshold):\n",
    "    if boxes.size(0) == 0:\n",
    "        return torch.empty(0, dtype=torch.int64, device=boxes.device)\n",
    "\n",
    "    # Sort scores in descending order\n",
    "    _, idxs = scores.sort(descending=True)\n",
    "    boxes = boxes[idxs]\n",
    "    scores = scores[idxs]\n",
    "\n",
    "    keep = []\n",
    "    while boxes.size(0) > 0:\n",
    "        pick = boxes.new_tensor([0], dtype=torch.long)\n",
    "        keep.append(idxs[pick].item())\n",
    "\n",
    "        iou = compute_3d_iou(boxes[pick].unsqueeze(0), boxes)\n",
    "        mask = (iou <= threshold).squeeze(0)\n",
    "\n",
    "        boxes = boxes[mask]\n",
    "        scores = scores[mask]\n",
    "        idxs = idxs[mask]\n",
    "\n",
    "    return torch.tensor(keep, dtype=torch.int64, device=boxes.device)\n",
    "\n",
    "\n",
    "def compute_3d_iou(box1, boxes):\n",
    "    if box1.dim() == 1:\n",
    "        box1 = box1.unsqueeze(0)\n",
    "    elif box1.dim() == 3:\n",
    "        box1 = box1.squeeze(0)\n",
    "\n",
    "    inter_xmin = torch.max(box1[:, 0].unsqueeze(1), boxes[:, 0])\n",
    "    inter_ymin = torch.max(box1[:, 1].unsqueeze(1), boxes[:, 1])\n",
    "    inter_zmin = torch.max(box1[:, 2].unsqueeze(1), boxes[:, 2])\n",
    "    inter_xmax = torch.min(box1[:, 3].unsqueeze(1), boxes[:, 3])\n",
    "    inter_ymax = torch.min(box1[:, 4].unsqueeze(1), boxes[:, 4])\n",
    "    inter_zmax = torch.min(box1[:, 5].unsqueeze(1), boxes[:, 5])\n",
    "\n",
    "    inter_volume = torch.clamp(inter_xmax - inter_xmin, min=0) * \\\n",
    "                   torch.clamp(inter_ymax - inter_ymin, min=0) * \\\n",
    "                   torch.clamp(inter_zmax - inter_zmin, min=0)\n",
    "\n",
    "    vol1 = (box1[:, 3] - box1[:, 0]) * (box1[:, 4] - box1[:, 1]) * (box1[:, 5] - box1[:, 2])\n",
    "    vol2 = (boxes[:, 3] - boxes[:, 0]) * (boxes[:, 4] - boxes[:, 1]) * (boxes[:, 5] - boxes[:, 2])\n",
    "\n",
    "    union_volume = vol1.unsqueeze(1) + vol2 - inter_volume\n",
    "\n",
    "    iou = inter_volume / union_volume.clamp(min=1e-6)\n",
    "\n",
    "    return iou\n",
    "# YOLO Loss Function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "   #     print(\"predictions\",predictions.shape)\n",
    "        batch_size = targets.size(0)\n",
    "        num_anchors = predictions.size(1)\n",
    "        grid_size_x = predictions.size(3)\n",
    "        grid_size_y = predictions.size(4)\n",
    "        grid_size_z = predictions.size(5)\n",
    "#        print(\"predictions\",predictions.shape)\n",
    "        # Reshape predictions correctly\n",
    "        predictions = predictions.view(batch_size, num_anchors, 7 + self.num_classes, grid_size_x, grid_size_y, grid_size_z)\n",
    " #       print(\"predictions\",predictions.shape)\n",
    "        # Ensure targets have the correct shape\n",
    "        targets = targets.view(batch_size, num_anchors, 7 + self.num_classes, grid_size_x, grid_size_y, grid_size_z)\n",
    "   #     print(\"targets\",targets.shape)\n",
    "\n",
    "        # Ensure predictions and targets have valid values\n",
    "        pred_boxes = predictions[:, :, :6, ...]  # Extract predicted boxes (x, y, z, w, h, d)\n",
    "        true_boxes = targets[:, :, :6, ...]      # Extract true boxes (x, y, z, w, h, d)\n",
    "\n",
    "        box_loss = self.mse_loss(pred_boxes, true_boxes)\n",
    "        pred_obj = torch.sigmoid(predictions[:, :, 6, ...])  # Predicted objectness score\n",
    "        true_obj = targets[:, :, 6, ...]  # True objectness score\n",
    "        obj_loss = self.bce_loss(pred_obj, true_obj)\n",
    "        pred_cls = predictions[:, :, 7:, ...]                # Predicted class probabilities\n",
    "        true_cls = targets[:, :, 7:, ...]                    # True class labels\n",
    "  #      print(true_cls.shape,\"true_cls\")\n",
    "   #     print(pred_cls.shape,\"pred_cls\")\n",
    "        class_loss = self.ce_loss(pred_cls.reshape(-1, self.num_classes), true_cls.reshape(-1, self.num_classes).argmax(dim=-1))\n",
    "        total_loss = box_loss + obj_loss + class_loss\n",
    "        return total_loss\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95ee42ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: torch.Size([2, 1, 146, 146, 146])\n",
      "Targets: torch.Size([2, 1, 8, 9, 9, 9])\n",
      "IMAGE SHAPE torch.Size([2, 1, 146, 146, 146])\n",
      "Image tensor dtype: torch.float32\n",
      "TARGET torch.Size([2, 1, 8, 9, 9, 9])\n",
      "Epoch [1/5], Batch [0/2], Loss: 0.7369\n",
      "IMAGE SHAPE torch.Size([2, 1, 146, 146, 146])\n",
      "Image tensor dtype: torch.float32\n",
      "TARGET torch.Size([2, 1, 8, 9, 9, 9])\n",
      "Epoch [1/5], Loss: 3.5355\n",
      "IMAGE SHAPE torch.Size([2, 1, 146, 146, 146])\n",
      "Image tensor dtype: torch.float32\n",
      "TARGET torch.Size([2, 1, 8, 9, 9, 9])\n",
      "Epoch [2/5], Batch [0/2], Loss: 1.3154\n",
      "IMAGE SHAPE torch.Size([2, 1, 146, 146, 146])\n",
      "Image tensor dtype: torch.float32\n",
      "TARGET torch.Size([2, 1, 8, 9, 9, 9])\n",
      "Epoch [2/5], Loss: 0.8432\n",
      "IMAGE SHAPE torch.Size([2, 1, 146, 146, 146])\n",
      "Image tensor dtype: torch.float32\n",
      "TARGET torch.Size([2, 1, 8, 9, 9, 9])\n",
      "Epoch [3/5], Batch [0/2], Loss: 0.6268\n",
      "IMAGE SHAPE torch.Size([2, 1, 146, 146, 146])\n",
      "Image tensor dtype: torch.float32\n",
      "TARGET torch.Size([2, 1, 8, 9, 9, 9])\n",
      "Epoch [3/5], Loss: 0.6007\n"
     ]
    }
   ],
   "source": [
    "class Yolo3DCustomDataset(Dataset):\n",
    "    def __init__(self, num_samples, num_classes = 1, img_size=146, grid_size=7, num_anchors=1):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_classes = num_classes\n",
    "        self.grid_size = grid_size\n",
    "        self.num_anchors = num_anchors\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate random 3D image data (numpy array: depth x height x width x channels)\n",
    "        img = np.random.randint(0, 256, (self.img_size, self.img_size, self.img_size, 1), dtype=np.uint8)\n",
    "        img = torch.tensor(img, dtype=torch.float32).permute(3, 0, 1, 2) / 255.0  # Normalize to [0, 1]\n",
    "        # Generate random bounding boxes and class labels\n",
    "        targets = torch.zeros((self.num_anchors, 7 + self.num_classes, self.grid_size, self.grid_size, self.grid_size))\n",
    "        for anchor in range(self.num_anchors):\n",
    "            x = np.random.uniform(0, self.img_size)\n",
    "            y = np.random.uniform(0, self.img_size)\n",
    "            z = np.random.uniform(0, self.img_size)\n",
    "            w = np.random.uniform(10, 50)  # Random width\n",
    "            h = np.random.uniform(10, 50)  # Random height\n",
    "            d = np.random.uniform(10, 50)  # Random depth\n",
    "            obj_score = 1  # Assuming there is an object\n",
    "            class_label = np.random.randint(0, self.num_classes)\n",
    "            class_one_hot = np.zeros(self.num_classes)\n",
    "            class_one_hot[class_label] = 1\n",
    "            # Determine which grid cell the object center falls into\n",
    "            grid_x = int(x // (self.img_size / self.grid_size))\n",
    "            grid_y = int(y // (self.img_size / self.grid_size))\n",
    "            grid_z = int(z // (self.img_size / self.grid_size))\n",
    "            # Normalize the coordinates and dimensions\n",
    "            x = (x % (self.img_size / self.grid_size)) / (self.img_size / self.grid_size)\n",
    "            y = (y % (self.img_size / self.grid_size)) / (self.img_size / self.grid_size)\n",
    "            z = (z % (self.img_size / self.grid_size)) / (self.img_size / self.grid_size)\n",
    "            w /= self.img_size\n",
    "            h /= self.img_size\n",
    "            d /= self.img_size\n",
    "            # Fill in the target tensor\n",
    "            targets[anchor, :6, grid_x, grid_y, grid_z] = torch.tensor([x, y, z, w, h, d])\n",
    "            targets[anchor, 6, grid_x, grid_y, grid_z] = obj_score\n",
    "            targets[anchor, 7:, grid_x, grid_y, grid_z] = torch.tensor(class_one_hot)\n",
    "        return img, targets\n",
    "num_classes = 1\n",
    "dataset = Yolo3DCustomDataset(num_samples=4, num_classes=num_classes, img_size=146, grid_size=9, num_anchors=1)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "# Iterate through the dataset\n",
    "for imgs, targets in dataloader:\n",
    "    print(\"Images:\", imgs.shape)       # Should be [batch_size, 1, img_size, img_size, img_size]\n",
    "    print(\"Targets:\", targets.shape)   # Should be [batch_size, num_anchors, 7 + num_classes, grid_size, grid_size, grid_size]\n",
    "    break\n",
    "num_classes = 1\n",
    "model = ResNet3D(BasicBlock3D, [2, 2, 2, 2], num_classes=num_classes)\n",
    "# Instantiate the loss function\n",
    "criterion = YoloLoss(num_classes=num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    total_batches = len(dataloader)\n",
    "    for batch_idx, (imgs, targets) in enumerate(dataloader):\n",
    "        print(\"IMAGE SHAPE\",imgs.shape)\n",
    "        print(\"Image tensor dtype:\", imgs.dtype)\n",
    "        print(\"TARGET\",targets.shape)\n",
    "        optimizer.zero_grad()\n",
    "        _, predictions = model(imgs)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print progress within epoch\n",
    "        if batch_idx % 10 == 0:  # Adjust frequency of printing as needed\n",
    "            print(f\"Epoch [{epoch + 1}/{5}], Batch [{batch_idx}/{total_batches}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{5}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2d663dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m img_height \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m146\u001b[39m\n\u001b[1;32m     87\u001b[0m img_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m156\u001b[39m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     89\u001b[0m grid_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m\n\u001b[1;32m     90\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/__init__.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/__init__.py:42\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# from tensorflow.python import keras\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# from tensorflow.python.layers import layers\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saved_model\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtpu\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Sub-package for performing i/o directly instead of via ops in a graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/saved_model/saved_model.py:20\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Convenience functions to save a model.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m builder\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loader\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/saved_model/builder.py:23\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"SavedModel builder.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mBuilds a SavedModel that can be saved to storage, is language neutral, and\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03menables systems to produce, consume, or transform TensorFlow Models.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder_impl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _SavedModelBuilder\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder_impl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SavedModelBuilder\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/saved_model/builder_impl.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpywrap_saved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpywrap_saved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saver \u001b[38;5;28;01mas\u001b[39;00m tf_saver\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated_args\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/training/saver.py:32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saver_pb2\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trackable_object_graph_pb2\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m checkpoint_management\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/checkpoint/checkpoint_management.py:30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m file_io\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variable_scope\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training_util\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/variable_scope.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/client/session.py:32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m error_interpolation\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m errors\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m indexed_slices\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/error_interpolation.py:34\u001b[0m\n\u001b[1;32m     31\u001b[0m _INTERPOLATION_REGEX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(?P<sep>.*?)(?P<tag>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_TAG_REGEX\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m _INTERPOLATION_PATTERN \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(_INTERPOLATION_REGEX, re\u001b[38;5;241m.\u001b[39mDOTALL)\n\u001b[0;32m---> 34\u001b[0m _ParseTag \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ParseTag\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Remove the last three path components from this module's file (i.e.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# python/framework/error_interpolation.py) so that we have an absolute path\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# prefix to the root of the installation.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m _FRAMEWORK_COMMON_PREFIX \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\n\u001b[1;32m     41\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/collections/__init__.py:379\u001b[0m, in \u001b[0;36mnamedtuple\u001b[0;34m(typename, field_names, rename, defaults, module)\u001b[0m\n\u001b[1;32m    377\u001b[0m     field_names \u001b[38;5;241m=\u001b[39m field_names\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m    378\u001b[0m field_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, field_names))\n\u001b[0;32m--> 379\u001b[0m typename \u001b[38;5;241m=\u001b[39m _sys\u001b[38;5;241m.\u001b[39mintern(\u001b[38;5;28mstr\u001b[39m(typename))\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rename:\n\u001b[1;32m    382\u001b[0m     seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_yolo_target_tensor_3d(bboxes_dict, img_width, img_height, img_depth, grid_size, num_classes):\n",
    "    \"\"\"\n",
    "    Transforms 3D bounding boxes and class labels into a YOLO target tensor for non-cubic volumes,\n",
    "    using the first bounding box as the anchor.\n",
    "\n",
    "    Parameters:\n",
    "    - bboxes_dict: Dictionary of bounding boxes, where key is the ID and value is a list of bounding boxes.\n",
    "    - img_width: Width of the input image.\n",
    "    - img_height: Height of the input image.\n",
    "    - img_depth: Depth of the input image.\n",
    "    - grid_size: Number of grid cells along each dimension (e.g., 13 or 19).\n",
    "    - num_classes: Total number of classes.\n",
    "\n",
    "    Returns:\n",
    "    - target_tensor: YOLO target tensor of shape (num_images, num_anchors, 7 + num_classes, grid_size, grid_size, grid_size).\n",
    "    \"\"\"\n",
    "    \n",
    "    num_images = len(bboxes_dict)\n",
    "    num_anchors = 1  # Single anchor for simplicity\n",
    "    \n",
    "    # Initialize the target tensor\n",
    "    target_tensor = torch.zeros((num_images, num_anchors, 7 + num_classes, grid_size, grid_size, grid_size))\n",
    "    \n",
    "    # Compute the cell sizes\n",
    "    cell_width = img_width / grid_size\n",
    "    cell_height = img_height / grid_size\n",
    "    cell_depth = img_depth / grid_size\n",
    "    \n",
    "    for img_idx, (img_id, bboxes) in enumerate(bboxes_dict.items()):\n",
    "        aggregated_bboxes = []\n",
    "        aggregated_classes = []\n",
    "        \n",
    "        # Flatten the list of lists to extract all bounding boxes\n",
    "        for bbox_list in bboxes:\n",
    "            for bbox in bbox_list:\n",
    "                if isinstance(bbox, list) and len(bbox) == 6:\n",
    "                    aggregated_bboxes.append(bbox)\n",
    "                    # Extract the class from the image ID, assuming the class label is in the format '#class_label'\n",
    "                    class_label = int(img_id.split('#')[1]) % num_classes\n",
    "                    aggregated_classes.append(class_label)\n",
    "        \n",
    "        # Use the first bounding box as the anchor\n",
    "        if aggregated_bboxes:\n",
    "            anchors = [aggregated_bboxes[0][3:6]]\n",
    "            print(f\"Image {img_id} Anchor: {anchors}\")  # Print the single anchor\n",
    "        \n",
    "        for bbox, class_label in zip(aggregated_bboxes, aggregated_classes):\n",
    "            x_center, y_center, z_center, width, height, depth = bbox\n",
    "            \n",
    "            # Calculate the grid cell coordinates\n",
    "            grid_x = int(x_center // cell_width)\n",
    "            grid_y = int(y_center // cell_height)\n",
    "            grid_z = int(z_center // cell_depth)\n",
    "            \n",
    "            # Calculate the relative coordinates within the grid cell\n",
    "            x_center_rel = (x_center % cell_width) / cell_width\n",
    "            y_center_rel = (y_center % cell_height) / cell_height\n",
    "            z_center_rel = (z_center % cell_depth) / cell_depth\n",
    "            width_rel = width / img_width\n",
    "            height_rel = height / img_height\n",
    "            depth_rel = depth / img_depth\n",
    "            \n",
    "            # Only use the anchor for the specific grid cell (9, 9, 9)\n",
    "            if (grid_x, grid_y, grid_z) == (9, 9, 9):\n",
    "                for anchor_idx, (anchor_width, anchor_height, anchor_depth) in enumerate(anchors):\n",
    "                    # Normalize the anchor dimensions\n",
    "                    anchor_width_rel = anchor_width / img_width\n",
    "                    anchor_height_rel = anchor_height / img_height\n",
    "                    anchor_depth_rel = anchor_depth / img_depth\n",
    "                    \n",
    "                    # Assign normalized bounding box values to the target tensor\n",
    "                    target_tensor[img_idx, anchor_idx, 0, grid_x, grid_y, grid_z] = x_center_rel\n",
    "                    target_tensor[img_idx, anchor_idx, 1, grid_x, grid_y, grid_z] = y_center_rel\n",
    "                    target_tensor[img_idx, anchor_idx, 2, grid_x, grid_y, grid_z] = z_center_rel\n",
    "                    target_tensor[img_idx, anchor_idx, 3, grid_x, grid_y, grid_z] = width_rel\n",
    "                    target_tensor[img_idx, anchor_idx, 4, grid_x, grid_y, grid_z] = height_rel\n",
    "                    target_tensor[img_idx, anchor_idx, 5, grid_x, grid_y, grid_z] = depth_rel\n",
    "                    target_tensor[img_idx, anchor_idx, 6, grid_x, grid_y, grid_z] = 1  # Object confidence score\n",
    "                    \n",
    "                    one_hot_class = torch.zeros(num_classes)\n",
    "                    one_hot_class[class_label] = 1\n",
    "                    target_tensor[img_idx, anchor_idx, 7:, grid_x, grid_y, grid_z] = one_hot_class\n",
    "    \n",
    "    return target_tensor\n",
    "img_width = 204\n",
    "img_height = 146\n",
    "img_depth = 156\n",
    "import tensorflow as tf\n",
    "grid_size = 9\n",
    "num_classes = 1\n",
    "y_train = create_yolo_target_tensor_3d(converted_bboxes_with_comments, img_width, img_height, img_depth, grid_size, num_classes)\n",
    "\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "# Define the folder containing the NIfTI images\n",
    "nifti_directory = '/Users/lucabernecker/Desktop/N128_local/aneu_det'\n",
    "\n",
    "# List all files in the folder\n",
    "file_ids = [key.strip(\"#\") for key in converted_bboxes_with_comments.keys()]\n",
    "\n",
    "# Initialize a list to store the loaded nifti images as numpy arrays\n",
    "nifti_images_list = []\n",
    "\n",
    "# Load the nifti images in the specified order\n",
    "for file_id in file_ids:\n",
    "    file_name = f\"cube_{file_id}.nii.gz\"\n",
    "    file_path = os.path.join(nifti_directory, file_name)\n",
    "    \n",
    "    # Load the nifti image\n",
    "    nifti_image = nib.load(file_path)\n",
    "    \n",
    "    # Convert the nifti image to a numpy array and append to the list\n",
    "    nifti_images_list.append(nifti_image.get_fdata())\n",
    "\n",
    "# Stack all numpy arrays into a single numpy array\n",
    "x_train = np.stack(nifti_images_list, axis=0)\n",
    "\n",
    "###############\n",
    "\n",
    "# Convert the list of images to a NumPy array\n",
    "print(\"X_train initial\",np.shape(x_train))\n",
    "x_train = torch.tensor(x_train)\n",
    "x_train = torch.unsqueeze(x_train, 1)\n",
    "print(\"X_Train\",np.shape(x_train))\n",
    "num_classes = 1\n",
    "model = ResNet3D(BasicBlock3D, [2, 2, 2, 2], num_classes = num_classes)\n",
    "#dataset = Yolo3DCustomDataset(num_samples=4, num_classes=num_classes,grid_size =8)\n",
    "#dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_train, y_train, new_shape=(146, 146, 146)):\n",
    "        \"\"\"\n",
    "        Custom dataset for YOLO-style object detection in 3D volumes.\n",
    "\n",
    "        Parameters:\n",
    "        - x_train (Tensor): Input images tensor of shape (num_images, channels, depth, height, width).\n",
    "        - y_train (Tensor): YOLO target tensor of shape (num_images, num_anchors, 7 + num_classes, grid_size, grid_size, grid_size).\n",
    "        - new_shape (tuple): Desired shape for resizing images (depth, height, width).\n",
    "        \"\"\"\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.new_shape = new_shape\n",
    "\n",
    "        assert len(self.x_train) == len(self.y_train), \\\n",
    "            \"Number of images in x_train must match number of target tensors in y_train.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x_train[idx]\n",
    "        y = self.y_train[idx]\n",
    "\n",
    "        # Reshape the image\n",
    "        x_resized = self.reshape_image(x, self.new_shape)\n",
    "        \n",
    "        # Rescale the bounding boxes in the targets\n",
    "        y_rescaled = self.rescale_bounding_boxes(y, x.shape[1:], self.new_shape)\n",
    "\n",
    "        return x_resized, y_rescaled\n",
    "\n",
    "    def reshape_image(self, image, new_shape):\n",
    "        \"\"\"\n",
    "        Reshape image tensor to new dimensions using trilinear interpolation.\n",
    "\n",
    "        Parameters:\n",
    "        - image (Tensor): Input image tensor of shape (channels, depth, height, width).\n",
    "        - new_shape (tuple): Desired shape for resizing (depth, height, width).\n",
    "\n",
    "        Returns:\n",
    "        - resized_image (Tensor): Resized image tensor of shape (channels, new_depth, new_height, new_width).\n",
    "        \"\"\"\n",
    "        resized_image = torch.nn.functional.interpolate(image.unsqueeze(0), size=new_shape, mode='trilinear', align_corners=False).squeeze(0)\n",
    "        return resized_image\n",
    "\n",
    "    def rescale_bounding_boxes(self, targets, original_shape, new_shape):\n",
    "        \"\"\"\n",
    "        Rescale bounding boxes in the targets according to the new image dimensions.\n",
    "\n",
    "        Parameters:\n",
    "        - targets (Tensor): YOLO target tensor of shape (num_anchors, 7 + num_classes, grid_size, grid_size, grid_size).\n",
    "        - original_shape (tuple): Original shape of the image (depth, height, width).\n",
    "        - new_shape (tuple): New shape of the image (depth, height, width).\n",
    "\n",
    "        Returns:\n",
    "        - rescaled_targets (Tensor): Target tensor with rescaled bounding boxes.\n",
    "        \"\"\"\n",
    "        # Calculate scaling factors for depth, height, and width\n",
    "        scale_z = new_shape[0] / original_shape[0]\n",
    "        scale_y = new_shape[1] / original_shape[1]\n",
    "        scale_x = new_shape[2] / original_shape[2]\n",
    "\n",
    "        # Rescale bounding boxes\n",
    "        targets_rescaled = targets.clone()\n",
    "        for anchor in targets_rescaled:\n",
    "            for bbox in anchor:\n",
    "                # Adjust bounding box center coordinates (x, y, z) and dimensions (w, h, d)\n",
    "                bbox[0] *= scale_x\n",
    "                bbox[1] *= scale_y\n",
    "                bbox[2] *= scale_z\n",
    "                bbox[3] *= scale_x\n",
    "                bbox[4] *= scale_y\n",
    "                bbox[5] *= scale_z\n",
    "\n",
    "        return targets_rescaled\n",
    "    \n",
    "dataset = CustomDataset(x_train,y_train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "for imgs, targets in dataloader:\n",
    "    print(\"Images:\", imgs.shape)       # Should be [batch_size, 1, img_size, img_size, img_size]\n",
    "    print(\"Targets:\", targets.shape)   # Should be [batch_size, num_anchors, 7 + num_classes, grid_size, grid_size, grid_size]\n",
    "    break\n",
    "# Instantiate the loss function\n",
    "criterion = YoloLoss(num_classes=num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    total_batches = len(dataloader)\n",
    "    for batch_idx, (imgs, targets) in enumerate(dataloader):\n",
    "      #  imgs = imgs.double()\n",
    "        imgs = imgs.float()\n",
    "      #  print(\"Image tensor dtype:\", imgs.dtype)\n",
    "      #  print(\"IMGS\",imgs.shape)\n",
    "      #  print(\"TARGETS\",targets.shape)\n",
    "        optimizer.zero_grad()\n",
    "        _, predictions = model(imgs)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print progress within epoch\n",
    "        if batch_idx % 10 == 0:  # Adjust frequency of printing as needed\n",
    "            print(f\"Epoch [{epoch + 1}/{5}], Batch [{batch_idx}/{total_batches}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{500}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b6e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 204, 146, 156]) image tensor\n",
      "torch.Size([1, 1, 146, 146, 146]) resized\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, image, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        _, prediction = model(image)\n",
    "        return prediction\n",
    "\n",
    "file_name = '/Users/lucabernecker/Desktop/N128_local/healthy_train/cube_100100.nii.gz'\n",
    "# List all files in the folder\n",
    "file_ids = [key.strip(\"#\") for key in converted_bboxes_with_comments.keys()]\n",
    "# Initialize a list to store the loaded nifti images as numpy arrays\n",
    "nifti_image = nib.load(file_name).get_fdata()\n",
    "image_tensor = torch.tensor(nifti_image)\n",
    "\n",
    "# Ensure the tensor is of type float32\n",
    "image_tensor = image_tensor.float()\n",
    "    # Convert the nifti image to a numpy array and append to the list\n",
    "image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "print(image_tensor.shape,\"image tensor\")\n",
    "resized_image = torch.nn.functional.interpolate(image_tensor.unsqueeze(0), size=(146,146,146), mode='trilinear', align_corners=False).squeeze(0)\n",
    "# Evaluate the model on the new image\n",
    "resized_image = resized_image.unsqueeze(0)\n",
    "print(resized_image.shape,\"resized\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "#new_image = new_image.to(device)\n",
    "\n",
    "prediction = evaluate_model(model, resized_image)\n",
    "print(\"Prediction on new image:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1057e409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'#100140': [[101.5, 85.0, 97.0, 9, 16, 16]], '#100151': [[120.0, 89.0, 87.5, 30, 12, 19]], '#100152': [[106.5, 79.5, 127.0, 11, 13, 14]], '#100171': [[64.0, 71.5, 107.0, 26, 13, 18]], '#100205': [[137.5, 70.0, 106.5, 21, 24, 29]], '#100218': [[121.0, 62.0, 112.0, 24, 12, 14]], '#100231': [[73.0, 96.5, 83.0, 18, 13, 14]], '#100234': [[82.5, 86.0, 86.5, 23, 12, 11]], '#100242': [[50.5, 87.5, 114.0, 21, 21, 22]], '#100277': [[122.5, 87.0, 130.5, 19, 20, 15]], '#100305': [[81.0, 86.0, 88.0, 18, 14, 16]], '#100316': [[182.5, 82.5, 111.0, 13, 13, 12]], '#100324': [[81.5, 93.0, 85.0, 19, 22, 28]], '#100326': [[99.5, 85.5, 109.5, 11, 19, 17]], '#100394': [[167.5, 93.0, 103.0, 11, 12, 12]], '#100428': [[120.5, 83.0, 116.5, 11, 10, 13]], '#100455': [[52.5, 82.0, 107.5, 13, 14, 19]], '#100460': [[186.0, 95.0, 111.0, 16, 12, 16]], '#100517': [[158.0, 86.0, 102.0, 32, 18, 20]], '#100536': [[32.5, 80.5, 95.5, 9, 27, 15]], '#100556': [[166.0, 87.5, 103.0, 14, 9, 18]], '#100601': [[147.0, 64.5, 142.5, 22, 13, 13]], '#100643': [[17.0, 79.0, 102.0, 16, 14, 14]], '#100649': [[97.5, 84.0, 102.5, 13, 14, 17]], '#100660': [[75.0, 84.5, 87.5, 16, 15, 13]], '#100673': [[134.5, 76.5, 103.0, 27, 19, 30]], '#100680': [[80.0, 80.0, 90.0, 24, 20, 24]], '#100700': [[98.5, 42.0, 110.5, 27, 28, 25]], '#100723': [[83.5, 92.0, 82.5, 27, 22, 33]], '#100739': [[17.0, 89.5, 108.0, 14, 15, 14]], '#100818': [[155.5, 85.0, 114.5, 23, 24, 17]], '#100873': [[110.5, 85.0, 95.0, 13, 20, 16]], '#100880': [[121.5, 66.0, 100.0, 19, 16, 12]], '#100915': [[112.0, 88.0, 78.0, 14, 16, 16]], '#100988': [[39.5, 95.5, 92.5, 19, 17, 17]], '#101026': [[53.5, 72.0, 113.0, 13, 18, 20]], '#101043': [[160.5, 86.0, 113.0, 7, 10, 18]], '#101054': [[28.5, 111.0, 97.5, 11, 12, 13]], '#101085': [[74.5, 96.5, 94.0, 19, 17, 18]], '#101118': [[120.0, 78.0, 93.5, 22, 12, 17]], '#101122': [[128.0, 66.5, 104.5, 12, 15, 13]], '#101140': [[125.0, 70.0, 97.5, 12, 14, 13]], '#101148': [[106.5, 55.5, 103.5, 9, 15, 17]], '#101157': [[46.5, 86.5, 89.0, 19, 9, 14]], '#101158': [[169.5, 78.0, 112.0, 17, 12, 14]], '#101194': [[184.0, 78.5, 108.0, 20, 13, 16]], '#101220': [[130.0, 68.0, 106.5, 12, 14, 15]], '#101234': [[120.5, 86.0, 91.0, 25, 20, 14]], '#101263': [[121.5, 87.0, 96.0, 17, 14, 32]], '#101272': [[19.5, 82.0, 100.0, 15, 8, 10]], '#101298': [[155.0, 96.0, 116.0, 12, 14, 14]], '#101302': [[41.5, 71.5, 104.5, 11, 15, 17]], '#101309': [[98.0, 78.0, 112.0, 14, 12, 10]], '#101331': [[97.5, 82.5, 106.5, 9, 13, 15]], '#101365': [[83.5, 82.0, 122.0, 17, 16, 16]], '#101393': [[182.5, 91.0, 106.0, 13, 26, 20]], '#101493': [[103.0, 41.0, 110.0, 24, 20, 20]], '#101495': [[151.0, 74.5, 115.5, 10, 19, 13]], '#101508': [[40.5, 91.5, 104.5, 13, 19, 13]], '#101516': [[12.0, 100.0, 130.0, 10, 18, 12]], '#101562': [[23.5, 88.5, 109.0, 17, 17, 14]], '#101601': [[171.0, 91.5, 100.5, 14, 15, 17]], '#101675': [[80.5, 84.0, 86.5, 19, 14, 17]], '#101735': [[42.0, 74.0, 121.0, 18, 12, 10]], '#101782': [[123.5, 83.5, 90.0, 17, 15, 12]], '#101842': [[42.0, 76.5, 93.0, 20, 23, 20]], '#101885': [[100.5, 85.0, 118.5, 13, 24, 23]], '#101892': [[145.0, 75.0, 112.5, 16, 12, 13]], '#101990': [[118.5, 82.5, 81.5, 35, 15, 19]], '#101991': [[106.0, 90.5, 113.0, 16, 15, 16]], '#101995': [[121.0, 72.0, 100.5, 14, 16, 11]], '#101998': [[120.0, 83.5, 103.0, 26, 17, 16]], '#102024': [[75.5, 64.5, 103.5, 19, 13, 21]], '#102084': [[48.0, 88.5, 98.0, 20, 25, 12]], '#102141': [[102.0, 86.5, 116.5, 12, 15, 13]], '#102158': [[157.5, 87.0, 108.0, 17, 16, 18]], '#102200': [[116.0, 92.0, 88.5, 30, 20, 33]], '#102201': [[117.5, 85.0, 96.0, 23, 16, 12]], '#102202': [[102.5, 80.0, 108.0, 15, 18, 10]], '#102228': [[156.0, 86.5, 105.5, 14, 9, 13]], '#102283': [[175.5, 85.0, 98.0, 13, 18, 16]], '#102333': [[114.0, 92.0, 100.0, 20, 20, 20]], '#102348': [[78.0, 83.5, 96.5, 14, 19, 13]], '#102391': [[128.5, 74.5, 108.0, 21, 17, 16]], '#102418': [[126.5, 78.5, 94.5, 19, 25, 17]], '#102427': [[78.5, 93.5, 89.5, 21, 13, 17]], '#102442': [[75.0, 73.5, 100.0, 16, 13, 16]], '#102455': [[152.0, 77.0, 92.5, 16, 12, 17]], '#102496': [[170.5, 82.0, 108.0, 21, 16, 16]], '#102564': [[127.5, 80.5, 113.5, 23, 21, 15]], '#102601': [[96.0, 88.5, 98.5, 16, 15, 11]], '#102609': [[31.0, 89.0, 104.5, 22, 18, 21]], '#102638': [[50.0, 78.0, 121.0, 20, 16, 18]], '#102693': [[167.0, 90.0, 83.5, -84, 16, 17]], '#102720': [[29.0, 93.0, 97.5, 14, 14, 11]], '#102761': [[76.5, 87.0, 89.0, 17, 14, 18]], '#102764': [[81.5, 82.0, 91.5, 19, 20, 11]], '#102797': [[79.5, 94.0, 91.0, 13, 16, 14]], '#102835': [[167.5, 97.0, 94.5, 21, 14, 13]], '#102840': [[120.0, 88.0, 86.0, 18, 12, 16]], '#102886': [[45.0, 82.5, 102.0, 8, 23, 10]], '#102935': [[152.0, 81.0, 112.0, 18, 20, 16]], '#102959': [[75.5, 85.5, 94.5, 19, 15, 9]], '#102974': [[118.5, 85.5, 80.5, 17, 19, 19]]}\n"
     ]
    }
   ],
   "source": [
    "def convert_bboxes(bboxes):\n",
    "    # Check if bboxes is a list of bounding boxes\n",
    "    if isinstance(bboxes[0][0], list):\n",
    "        # Convert each bounding box in the list\n",
    "        return [convert_bboxes(bbox) for bbox in bboxes]\n",
    "    else:\n",
    "        # Convert a single bounding box\n",
    "        x_min, x_max = bboxes[0]\n",
    "        y_min, y_max = bboxes[1]\n",
    "        z_min, z_max = bboxes[2]\n",
    "\n",
    "        x_center = (x_min + x_max) / 2\n",
    "        y_center = (y_min + y_max) / 2\n",
    "        z_center = (z_min + z_max) / 2\n",
    "\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "        depth = z_max - z_min\n",
    "\n",
    "        return [x_center, y_center, z_center, width, height, depth]\n",
    "# Example usage\n",
    "bounding_boxes_with_comments = {\n",
    "    \"#100140\": [[[97, 106], [77, 93], [89, 105]]],\n",
    "    \"#100151\": [[[105, 135], [83, 95], [78, 97]]],\n",
    "    \"#100152\": [[[101, 112], [73, 86], [120, 134]]],\n",
    "    \"#100171\": [[[51, 77], [65, 78], [98, 116]]],\n",
    "    \"#100205\": [[[127, 148], [58, 82], [92, 121]]],\n",
    "    \"#100218\": [[[109, 133], [56, 68], [105, 119]]],\n",
    "    \"#100231\": [[[64, 82], [90, 103], [76, 90]]],\n",
    "    \"#100234\": [[[71, 94], [80, 92], [81, 92]]],\n",
    "    \"#100242\": [[[40, 61], [77, 98], [103, 125]]],\n",
    "    \"#100277\": [[[113, 132], [77, 97], [123, 138]]],\n",
    "    \"#100305\": [[[72, 90], [79, 93], [80, 96]]],\n",
    "    \"#100316\": [[[176, 189], [76, 89], [105, 117]]],\n",
    "    \"#100324\": [[[72, 91], [82, 104], [71, 99]]],\n",
    "    \"#100326\": [[[94, 105], [76, 95], [101, 118]]],\n",
    "    \"#100394\": [[[162, 173], [87, 99], [97, 109]]],\n",
    "   # \"#100248\": [[[115, 129], [76, 94], [110, 124]]],\n",
    "    \"#100428\": [[[115,126],[78,88],[110,123]]],\n",
    "    \"#100455\": [[[46, 59], [75, 89], [98, 117]]],\n",
    "    \"#100460\": [[[178, 194], [89, 101], [103, 119]]],\n",
    "    \"#100517\": [[[142, 174], [77, 95], [92, 112]]],\n",
    "    \"#100536\": [[[28, 37], [67, 94], [88, 103]]],\n",
    "    \"#100556\": [[[159, 173], [83, 92], [94, 112]]],\n",
    "    \"#100601\": [[[136, 158], [58, 71], [136, 149]]],\n",
    "    \"#100643\": [[[9, 25], [72, 86], [95, 109]]],\n",
    "    \"#100649\": [[[91, 104], [77, 91], [94, 111]]],\n",
    "    \"#100660\": [[[67, 83], [77, 92], [81, 94]]],\n",
    "    \"#100673\": [[[121, 148], [67, 86], [88, 118]]],\n",
    "    \"#100680\": [[[68, 92], [70, 90], [78, 102]]],\n",
    "    \"#100700\": [[[85, 112], [28, 56], [98, 123]]],\n",
    "    \"#100723\": [[[70, 97], [81, 103], [66, 99]]],\n",
    "    \"#100739\": [[[10, 24], [82, 97], [101, 115]]],\n",
    "    \"#100818\": [[[144, 167], [73, 97], [106, 123]]],\n",
    "    \"#100873\": [[[104, 117], [75, 95], [87, 103]]],\n",
    "    \"#100880\": [[[112, 131], [58, 74], [94, 106]]],\n",
    "    \"#100915\": [[[105, 119], [80, 96], [70, 86]]],\n",
    "    \"#100988\": [[[30, 49], [87, 104], [84, 101]]],\n",
    "    \"#101026\": [[[47, 60], [63, 81], [103, 123]]],\n",
    "    \"#101043\": [[[157, 164], [81, 91], [104, 122]]],\n",
    "    \"#101054\": [[[23, 34], [105, 117], [91, 104]]],\n",
    "    \"#101085\": [[[65, 84], [88, 105], [85, 103]]],\n",
    "    \"#101118\": [[[109, 131], [72, 84], [85, 102]]],\n",
    "    \"#101122\": [[[122, 134], [59, 74], [98, 111]]],\n",
    "    \"#101140\": [[[119, 131], [63, 77], [91, 104]]],\n",
    "    \"#101148\": [[[102, 111], [48, 63], [95, 112]]],\n",
    "    \"#101157\": [[[37, 56], [82, 91], [82, 96]]],\n",
    "    \"#101158\": [[[161, 178], [72, 84], [105, 119]]],\n",
    "    \"#101194\": [[[174, 194], [72, 85], [100, 116]]],\n",
    "    \"#101220\": [[[124, 136], [61, 75], [99, 114]]],\n",
    "    \"#101234\": [[[108, 133], [76, 96], [84, 98]]],\n",
    "    \"#101263\": [[[113, 130], [80, 94], [80, 112]]],\n",
    "    \"#101272\": [[[12, 27], [78, 86], [95, 105]]],\n",
    " #   \"#101288\": [[[35, 48], [86, 103], [90, 104]],[[142,154], [66,78], [100,113]]],\n",
    "    \"#101298\": [[[149, 161], [89, 103], [109, 123]]],\n",
    "    \"#101302\": [[[36, 47], [64, 79], [96, 113]]],\n",
    "    \"#101309\": [[[91, 105], [72, 84], [107, 117]]],\n",
    "    \"#101331\": [[[93, 102], [76, 89], [99, 114]]],\n",
    "    \"#101365\": [[[75, 92], [74, 90], [114, 130]]],\n",
    "    \"#101393\": [[[176, 189], [78, 104], [96, 116]]],\n",
    " #   \"#101426\": [[[12, 23], [65, 90], [88, 98]],[[106,115],[45,57],[104,115]]],  \n",
    "    \"#101493\": [[[91, 115], [31, 51], [100, 120]]],\n",
    "    \"#101495\": [[[146, 156], [65, 84], [109, 122]]],\n",
    "    \"#101508\": [[[34, 47], [82, 101], [98, 111]]],\n",
    "    \"#101516\": [[[7, 17], [91, 109], [124, 136]]],\n",
    "    \"#101562\": [[[15, 32], [80, 97], [102, 116]]],\n",
    "    \"#101601\": [[[164, 178], [84, 99], [92, 109]]],\n",
    "    \"#101675\": [[[71, 90], [77, 91], [78, 95]]],\n",
    "    \"#101735\": [[[33, 51], [68, 80],[116,126]]],\n",
    "    \"#101782\": [[[115, 132], [76, 91], [84, 96]]],\n",
    "    \"#101842\": [[[32, 52], [65, 88], [83, 103]]],\n",
    "    \"#101885\": [[[94, 107], [73, 97], [107, 130]]],\n",
    "    \"#101892\": [[[137, 153], [69, 81], [106, 119]]],\n",
    "    \"#101990\": [[[101, 136], [75, 90], [72, 91]]],\n",
    "    \"#101991\": [[[98, 114], [83, 98], [105, 121]]],\n",
    "    \"#101995\": [[[114, 128], [64, 80], [95, 106]]],\n",
    "    \"#101998\": [[[107, 133], [75, 92], [95, 111]]],\n",
    "    \"#102024\": [[[66, 85], [58, 71], [93, 114]]],\n",
    "    \"#102084\": [[[38, 58], [76, 101], [92, 104]]],\n",
    "    \"#102141\": [[[96, 108], [79, 94], [110, 123]]],\n",
    "    \"#102158\": [[[149, 166], [79, 95], [99, 117]]],\n",
    "    \"#102200\": [[[101, 131], [82, 102], [72, 105]]],\n",
    "    \"#102201\": [[[106, 129], [77, 93], [90, 102]]],\n",
    "    \"#102202\": [[[95, 110], [71, 89], [103, 113]]],\n",
    "    \"#102228\": [[[149, 163], [82, 91], [99, 112]]],\n",
    "    \"#102283\": [[[169, 182], [76, 94], [90, 106]]],\n",
    "    \"#102333\": [[[104, 124], [82, 102], [90, 110]]],\n",
    "    \"#102348\": [[[71, 85], [74, 93], [90, 103]]],\n",
    "    \"#102391\": [[[118, 139], [66, 83], [100, 116]]],\n",
    "    \"#102418\": [[[117, 136], [66, 91], [86, 103]]],\n",
    "    \"#102427\": [[[68, 89], [87, 100], [81, 98]]],\n",
    "    \"#102442\": [[[67, 83], [67, 80], [92, 108]]],\n",
    "    \"#102455\": [[[144, 160], [71, 83], [84, 101]]],\n",
    "    \"#102496\": [[[160, 181], [74, 90], [100, 116]]],\n",
    "    \"#102564\": [[[116, 139], [70, 91], [106, 121]]],\n",
    "#    \"#102590\": [[[150, 164], [81, 90], [93, 109]],[[114,133],[78,92],[79,88]]],  # and [[118, 136], [82, 93], [77, 91]]\n",
    "#    \"#102595\": [[[112, 127], [70, 91], [83, 93]],[[113, 131], [75, 86], [92, 105]]],  # and [[113, 131], [75, 86], [92, 105]]\n",
    "    \"#102601\": [[[88, 104], [81, 96], [93, 104]]],\n",
    "    \"#102609\": [[[20, 42], [80, 98], [94, 115]]],\n",
    "    \"#102638\": [[[40, 60], [70, 86], [112, 130]]],\n",
    "    \"#102693\": [[[209, 125], [82, 98], [75, 92]]],\n",
    "    \"#102720\": [[[22, 36], [86, 100], [92, 103]]],\n",
    "    \"#102761\": [[[68, 85], [80, 94], [80, 98]]],\n",
    "    \"#102764\": [[[72, 91], [72, 92], [86, 97]]],\n",
    "    \"#102797\": [[[73, 86], [86, 102], [84, 98]]],\n",
    "    \"#102835\": [[[157, 178], [90, 104], [88, 101]]],\n",
    "    \"#102840\": [[[111, 129], [82, 94], [78, 94]]],\n",
    "  #  \"#102864\": [[[112, 132], [79, 100], [79, 93]],[[68, 88], [83, 97], [82, 95]],[[184, 201], [32, 48], [131, 143]]],  # and [[68, 88], [83, 97], [82, 95]], [[184, 201], [32, 48], [131, 143]]\n",
    "    \"#102886\": [[[41, 49], [71, 94], [97, 107]]],\n",
    "    \"#102935\": [[[143, 161], [71, 91], [104, 120]]],\n",
    "    \"#102959\": [[[66, 85], [78, 93], [90, 99]]],\n",
    "    \"#102974\": [[[110, 127], [76, 95], [71, 90]]]\n",
    "}\n",
    "\n",
    "\n",
    "converted_bboxes_with_comments = {}\n",
    "\n",
    "for key, bboxes in bounding_boxes_with_comments.items():\n",
    "    # Convert each list of bounding boxes or single bounding box\n",
    "    converted_bboxes_with_comments[key] = convert_bboxes(bboxes)\n",
    "\n",
    "print(converted_bboxes_with_comments)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946ac32",
   "metadata": {},
   "source": [
    "# Anchor Free Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe4cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3157e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.715826511383057\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Detect(nn.Module):\n",
    "    def __init__(self, num_classes, conf_threshold=0.5, nms_threshold=0.4):\n",
    "        super(Detect, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.nms_threshold = nms_threshold\n",
    "\n",
    "    def forward(self, predictions):\n",
    "        batch_size = predictions.size(0)\n",
    "        grid_size = predictions.size(2)\n",
    "        \n",
    "        # Adjust number of channels for anchor-free\n",
    "        num_params = 6 + 1 + self.num_classes  # 6 bbox, 1 obj confidence, num_classes\n",
    "        predictions = predictions.view(batch_size, num_params, grid_size, grid_size, grid_size)\n",
    "\n",
    "        # Predictions\n",
    "        bbox_pred = predictions[:, :6, :, :, :]  # bbox predictions (x, y, z, w, h, d)\n",
    "        obj_confidence = torch.sigmoid(predictions[:, 6, :, :, :])  # object confidence\n",
    "        class_scores = torch.sigmoid(predictions[:, 7:, :, :, :])  # class scores\n",
    "\n",
    "        # Adjust for center heatmap\n",
    "        grid_x = torch.arange(grid_size, dtype=torch.float, device=predictions.device).repeat(grid_size, grid_size, 1)\n",
    "        grid_y = torch.arange(grid_size, dtype=torch.float, device=predictions.device).repeat(grid_size, grid_size, 1).transpose(1, 2)\n",
    "        grid_z = torch.arange(grid_size, dtype=torch.float, device=predictions.device).repeat(grid_size, grid_size, 1).transpose(0, 1)\n",
    "\n",
    "        scaled_anchors = torch.zeros_like(bbox_pred)\n",
    "\n",
    "        scaled_anchors[:, 0, :, :, :] = (torch.sigmoid(bbox_pred[:, 0, :, :, :]) + grid_x) / grid_size\n",
    "        scaled_anchors[:, 1, :, :, :] = (torch.sigmoid(bbox_pred[:, 1, :, :, :]) + grid_y) / grid_size\n",
    "        scaled_anchors[:, 2, :, :, :] = (torch.sigmoid(bbox_pred[:, 2, :, :, :]) + grid_z) / grid_size\n",
    "        scaled_anchors[:, 3, :, :, :] = torch.exp(bbox_pred[:, 3, :, :, :]) / grid_size\n",
    "        scaled_anchors[:, 4, :, :, :] = torch.exp(bbox_pred[:, 4, :, :, :]) / grid_size\n",
    "        scaled_anchors[:, 5, :, :, :] = torch.exp(bbox_pred[:, 5, :, :, :]) / grid_size\n",
    "\n",
    "        x_min = scaled_anchors[:, 0, :, :, :] - scaled_anchors[:, 3, :, :, :] / 2\n",
    "        y_min = scaled_anchors[:, 1, :, :, :] - scaled_anchors[:, 4, :, :, :] / 2\n",
    "        z_min = scaled_anchors[:, 2, :, :, :] - scaled_anchors[:, 5, :, :, :] / 2\n",
    "        x_max = scaled_anchors[:, 0, :, :, :] + scaled_anchors[:, 3, :, :, :] / 2\n",
    "        y_max = scaled_anchors[:, 1, :, :, :] + scaled_anchors[:, 4, :, :, :] / 2\n",
    "        z_max = scaled_anchors[:, 2, :, :, :] + scaled_anchors[:, 5, :, :, :] / 2\n",
    "\n",
    "        detections = []\n",
    "        for batch_idx in range(batch_size):\n",
    "            batch_detections = []\n",
    "            for class_idx in range(self.num_classes):\n",
    "                class_scores_batch = class_scores[batch_idx, class_idx, :, :, :]\n",
    "                conf_scores = obj_confidence[batch_idx, :, :, :]\n",
    "                scores = conf_scores * class_scores_batch\n",
    "                mask = scores >= self.conf_threshold\n",
    "                scores = scores[mask]\n",
    "                if scores.size(0) == 0:\n",
    "                    continue\n",
    "                x_min_class = x_min[batch_idx, :, :, :][mask]\n",
    "                y_min_class = y_min[batch_idx, :, :, :][mask]\n",
    "                z_min_class = z_min[batch_idx, :, :, :][mask]\n",
    "                x_max_class = x_max[batch_idx, :, :, :][mask]\n",
    "                y_max_class = y_max[batch_idx, :, :, :][mask]\n",
    "                z_max_class = z_max[batch_idx, :, :, :][mask]\n",
    "                boxes = torch.stack((x_min_class, y_min_class, z_min_class, x_max_class, y_max_class, z_max_class), dim=-1)\n",
    "\n",
    "                keep = custom_3d_nms(boxes, scores, self.nms_threshold)\n",
    "\n",
    "                boxes = boxes.view(-1, 6)[keep]\n",
    "                scores = scores[keep]\n",
    "\n",
    "                batch_detections.extend(\n",
    "                    [{\"boxes\": boxes, \"scores\": scores, \"labels\": torch.tensor([class_idx]*len(boxes), device=predictions.device)}]\n",
    "                )\n",
    "            detections.append(batch_detections)\n",
    "\n",
    "        return detections, predictions\n",
    "\n",
    "def custom_3d_nms(boxes, scores, threshold):\n",
    "    if boxes.size(0) == 0:\n",
    "        return torch.empty(0, dtype=torch.int64, device=boxes.device)\n",
    "\n",
    "    _, idxs = scores.sort(descending=True)\n",
    "    boxes = boxes[idxs]\n",
    "    scores = scores[idxs]\n",
    "\n",
    "    keep = []\n",
    "    while boxes.size(0) > 0:\n",
    "        pick = boxes.new_tensor([0], dtype=torch.long)\n",
    "        keep.append(idxs[pick].item())\n",
    "\n",
    "        iou = compute_3d_iou(boxes[pick].unsqueeze(0), boxes)\n",
    "        mask = (iou <= threshold).squeeze(0)\n",
    "\n",
    "        boxes = boxes[mask]\n",
    "        scores = scores[mask]\n",
    "        idxs = idxs[mask]\n",
    "\n",
    "    return torch.tensor(keep, dtype=torch.int64, device=boxes.device)\n",
    "\n",
    "def compute_3d_iou(box1, boxes):\n",
    "    if box1.dim() == 1:\n",
    "        box1 = box1.unsqueeze(0)\n",
    "    elif box1.dim() == 3:\n",
    "        box1 = box1.squeeze(0)\n",
    "\n",
    "    inter_xmin = torch.max(box1[:, 0].unsqueeze(1), boxes[:, 0])\n",
    "    inter_ymin = torch.max(box1[:, 1].unsqueeze(1), boxes[:, 1])\n",
    "    inter_zmin = torch.max(box1[:, 2].unsqueeze(1), boxes[:, 2])\n",
    "    inter_xmax = torch.min(box1[:, 3].unsqueeze(1), boxes[:, 3])\n",
    "    inter_ymax = torch.min(box1[:, 4].unsqueeze(1), boxes[:, 4])\n",
    "    inter_zmax = torch.min(box1[:, 5].unsqueeze(1), boxes[:, 5])\n",
    "\n",
    "    inter_volume = torch.clamp(inter_xmax - inter_xmin, min=0) * \\\n",
    "                   torch.clamp(inter_ymax - inter_ymin, min=0) * \\\n",
    "                   torch.clamp(inter_zmax - inter_zmin, min=0)\n",
    "\n",
    "    vol1 = (box1[:, 3] - box1[:, 0]) * (box1[:, 4] - box1[:, 1]) * (box1[:, 5] - box1[:, 2])\n",
    "    vol2 = (boxes[:, 3] - boxes[:, 0]) * (boxes[:, 4] - boxes[:, 1]) * (boxes[:, 5] - boxes[:, 2])\n",
    "\n",
    "    union_volume = vol1.unsqueeze(1) + vol2 - inter_volume\n",
    "\n",
    "    iou = inter_volume / union_volume.clamp(min=1e-6)\n",
    "\n",
    "    return iou\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        batch_size = targets.size(0)\n",
    "        num_params = 6 + 1 + self.num_classes  # 6 bbox, 1 obj confidence, num_classes\n",
    "        grid_size_x = predictions.size(2)\n",
    "        grid_size_y = predictions.size(3)\n",
    "        grid_size_z = predictions.size(4)\n",
    "        \n",
    "        predictions = predictions.view(batch_size, num_params, grid_size_x, grid_size_y, grid_size_z)\n",
    "        targets = targets.view(batch_size, num_params, grid_size_x, grid_size_y, grid_size_z)\n",
    "\n",
    "        pred_boxes = predictions[:, :6, ...]\n",
    "        true_boxes = targets[:, :6, ...]\n",
    "\n",
    "        box_loss = self.mse_loss(pred_boxes, true_boxes)\n",
    "        pred_obj = torch.sigmoid(predictions[:, 6, ...])\n",
    "        true_obj = torch.sigmoid(targets[:, 6, ...])  # Ensure target values are between 0 and 1\n",
    "        obj_loss = self.bce_loss(pred_obj, true_obj)\n",
    "        pred_cls = predictions[:, 7:, ...]\n",
    "        true_cls = targets[:, 7:, ...]\n",
    "\n",
    "        class_loss = self.ce_loss(pred_cls.permute(0, 2, 3, 4, 1).reshape(-1, self.num_classes), true_cls.argmax(dim=1).reshape(-1))\n",
    "\n",
    "        total_loss = box_loss + obj_loss + class_loss\n",
    "        return total_loss\n",
    "\n",
    "# Example usage\n",
    "model = Detect(num_classes=80)\n",
    "loss_fn = YoloLoss(num_classes=80)\n",
    "\n",
    "predictions = torch.randn((1, 7 + 80, 13, 13, 13))  # Example predictions\n",
    "\n",
    "# Example targets with values within the expected range\n",
    "targets = torch.zeros((1, 7 + 80, 13, 13, 13))\n",
    "targets[0, :6, 5, 5, 5] = 0.5  # Example bounding box target\n",
    "targets[0, 6, 5, 5, 5] = 1  # Object confidence target\n",
    "targets[0, 7, 5, 5, 5] = 1  # Class target\n",
    "\n",
    "detections, _ = model(predictions)\n",
    "loss = loss_fn(predictions, targets)\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c80c239d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e039560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7e31424",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'converted_bboxes_with_comments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m nifti_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/lucabernecker/Desktop/N128_local/aneu_det\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# List all files in the folder\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m file_ids \u001b[38;5;241m=\u001b[39m [key\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m converted_bboxes_with_comments\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize a list to store the loaded nifti images as numpy arrays\u001b[39;00m\n\u001b[1;32m      7\u001b[0m nifti_images_list \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'converted_bboxes_with_comments' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8797f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
